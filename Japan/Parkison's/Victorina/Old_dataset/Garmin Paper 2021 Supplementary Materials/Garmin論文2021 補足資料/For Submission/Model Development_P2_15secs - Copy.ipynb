{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'participant2'\n",
    "frequency = '15s' # 15min | 15s\n",
    "dataset_type = '' # ''\n",
    "\n",
    "if frequency == '15min':\n",
    "    record_size_per_day = 96\n",
    "elif frequency == '15s':\n",
    "    record_size_per_day = 5760\n",
    "\n",
    "# Columns to include    \n",
    "if dataset_type == '':\n",
    "    columns = [ 'timestamp', 'heart_rate', 'steps', 'stress_score',\n",
    "            'awake', 'deep', 'light', 'rem', 'nonrem_total', 'total', 'nonrem_percentage', 'sleep_efficiency',\n",
    "            'time_from_last_drug_taken', 'wearing_off' ]\n",
    "\n",
    "metrics = {\n",
    "    'balanced_accuracy': 'Bal. Acc.',\n",
    "    'f1_score': 'F1 Score',\n",
    "    'accuracy': 'Acc.',\n",
    "    'precision': 'Precision',\n",
    "    'sensitivity': 'Recall / Sn',\n",
    "    'specificity': 'Sp',\n",
    "    'auc': 'AUC'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Union, Generator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "\n",
    "import sklearn\n",
    "from photonai.base import Hyperpipe, PipelineElement, Stack, Switch\n",
    "from photonai.optimization import FloatRange, IntegerRange, Categorical, BooleanSwitch, PhotonHyperparam\n",
    "from photonai.optimization import Categorical as PhotonCategorical\n",
    "from photonai.optimization import MinimumPerformanceConstraint, DummyPerformanceConstraint, BestPerformanceConstraint\n",
    "from photonai.optimization.base_optimizer import PhotonSlaveOptimizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate\n",
    "\n",
    "from skopt import Optimizer\n",
    "from skopt.space import Real, Integer, Dimension\n",
    "from skopt.space import Categorical as skoptCategorical\n",
    "from photonai.photonlogger.logger import logger\n",
    "from photonai.optimization.scikit_optimize.sk_opt import SkOptOptimizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "from photonai.base import Hyperpipe\n",
    "from photonai.optimization import MinimumPerformanceConstraint\n",
    "from photonai.photonlogger import logger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_excel(f'./data/combined_data/{dataset_type}combined_data_{user}_{frequency}.xlsx',\n",
    "                              index_col=\"timestamp\",\n",
    "                              usecols=columns,\n",
    "                              engine='openpyxl')\n",
    "if dataset_type == '':\n",
    "    # Define prediction horizon\n",
    "    predict_ahead = pd.Timedelta(minutes=60)  # <- change to 2 or pd.Timedelta(seconds=15) as needed\n",
    "\n",
    "    # Ensure datetime index\n",
    "    combined_data = combined_data.sort_index()\n",
    "    combined_data.index = pd.to_datetime(combined_data.index)\n",
    "\n",
    "    # Estimate time delta between rows (assumes regular sampling)\n",
    "    median_step = combined_data.index.to_series().diff().median()\n",
    "\n",
    "    # How many steps to shift based on prediction horizon\n",
    "    steps_ahead = int(predict_ahead / median_step)\n",
    "\n",
    "    # Shift labels backward to align X at time t with y at t + n\n",
    "    combined_data['wearing_off_future'] = combined_data['wearing_off'].shift(-steps_ahead)\n",
    "\n",
    "    # Drop rows with NaNs from shifting\n",
    "    combined_data = combined_data.dropna(subset=['wearing_off_future'])\n",
    "\n",
    "    # Define X and y\n",
    "    y = combined_data['wearing_off_future'].astype(int).values\n",
    "    X = combined_data.loc[:, columns[1:-1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Show feature importances\n",
    "def print_feature_importances(pipeline):\n",
    "    output = ''\n",
    "    if pipeline.optimum_pipe.feature_importances_ is None:\n",
    "        output = 'Best Hyperparameter Configuration is a non-linear SVM, thus feature importances cannot be retrieved'\n",
    "    else:\n",
    "        output = 'Feature Importances using the Best Hyperparameter Config'\n",
    "        if not [value.base_element.support_.tolist() for key, value in pipeline.optimum_pipe.elements if key == 'RFE']:\n",
    "            if len(pipeline.optimum_pipe.feature_importances_) == 1:\n",
    "                feature_importances = pipeline.optimum_pipe.feature_importances_[0]\n",
    "            else:\n",
    "                feature_importances = pipeline.optimum_pipe.feature_importances_\n",
    "            output += '\\n'\n",
    "            output += tabulate(\n",
    "                pd.DataFrame(\n",
    "                    [np.around(feature_importances, decimals=4)],\n",
    "                    columns=np.array(columns[1:-1])\n",
    "                ).transpose().sort_values(by=[0], ascending=False, key=abs),\n",
    "                tablefmt='psql', floatfmt=\".4f\", headers=['Features', 'Values']\n",
    "            )\n",
    "        else:\n",
    "            mask = [value.base_element.support_.tolist() for key, value in pipeline.optimum_pipe.elements if key == 'RFE'][0]\n",
    "            if len(pipeline.optimum_pipe.feature_importances_) == 1:\n",
    "                feature_importances = pipeline.optimum_pipe.feature_importances_[0]\n",
    "            else:\n",
    "                feature_importances = pipeline.optimum_pipe.feature_importances_\n",
    "            output += '\\n'\n",
    "            output += tabulate(\n",
    "                pd.DataFrame(\n",
    "                    [np.around(feature_importances, decimals=4)],\n",
    "                    columns=np.array(columns[1:-1])[mask]\n",
    "                ).transpose().sort_values(by=[0], ascending=False, key=abs),\n",
    "                tablefmt='psql', floatfmt=\".4f\", headers=['Features', 'Values']\n",
    "            )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, ax, n_splits=0, lw=20):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                    c=indices, marker='_', lw=lw, cmap=plt.cm.coolwarm,\n",
    "                    vmin=-.2, vmax=1.2)\n",
    "\n",
    "    n_splits = ii + 1\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['wearing-off']\n",
    "    ax.set(yticks=np.arange(n_splits+1) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Records\\'s Index', ylabel=\"Folds\",\n",
    "           ylim=[n_splits+1.2, -.2], xlim=[0, len(X)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_from_pipeline(pipeline):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cm = confusion_matrix(\n",
    "        pipeline.results_handler.get_test_predictions()['y_true'],\n",
    "        pipeline.results_handler.get_test_predictions()['y_pred'],\n",
    "        labels=[0,1], normalize=None)\n",
    "    ax = plt.subplot()\n",
    "    sns.set(font_scale=3.0) # Adjust to fit\n",
    "    sns.heatmap(cm, annot=True, ax=ax, cmap=\"Blues\", fmt=\"g\");  \n",
    "\n",
    "    # Labels, title and ticks\n",
    "    label_font = {'size':'25'}  # Adjust to fit\n",
    "    ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "    ax.set_ylabel('Observed labels', fontdict=label_font);\n",
    "\n",
    "    # title_font = {'size':'21'}  # Adjust to fit\n",
    "    # ax.set_title('Confusion Matrix', fontdict=title_font);\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=25)  # Adjust to fit\n",
    "    ax.xaxis.set_ticklabels(['Good', 'Wearing-Off']);\n",
    "    ax.yaxis.set_ticklabels(['Good', 'Wearing-Off']);\n",
    "    plt.rc('text') # , usetex=False)\n",
    "    plt.rc('font', family='serif')\n",
    "    # plt.savefig('./participant2-downsampling-confusionmatrix-real.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write other reports to summary file\n",
    "def add_other_report_to_summary(pipeline, with_estimator_comparison=True):\n",
    "    with open(f'{pipeline.output_settings.results_folder}/photon_summary.txt', \"a+\") as summary_file:\n",
    "        # 1. Write comparison of learning algorithms\n",
    "        if with_estimator_comparison:\n",
    "            summary_file.write(\"\\n\\n\")\n",
    "            summary_file.write(\"Comparison on learning algorithms on validation set\")\n",
    "            summary_file.write(\"\\n\")\n",
    "            summary_file.write(str(pipeline.results_handler.get_mean_of_best_validation_configs_per_estimator()))\n",
    "\n",
    "        # 2. Write feature importance\n",
    "        summary_file.write(\"\\n\\n\")\n",
    "        summary_file.write(\"Feature Importance\")\n",
    "        summary_file.write(print_feature_importances(pipeline))\n",
    "        \n",
    "        # 3. Write beautified average test performance across outer folds\n",
    "        # a. Get Average Test Performance Across Outer Folds\n",
    "        test_metric_result = pipeline.results.get_test_metric_dict()\n",
    "        \n",
    "        # b. Replace display metric name\n",
    "        #   Reference: https://stackoverflow.com/a/55250496/2303766\n",
    "        test_metric_result = { metrics[metric]: test_metric_result[metric]\n",
    "                                  for metric, metric_name in metrics.items() if metric in test_metric_result\n",
    "                             }\n",
    "        \n",
    "        # c. Add beautified average test performance across outer folds to file \n",
    "        summary_file.write(\"\\n\\n\")\n",
    "        summary_file.write(\"Average Test Performance Across Outer Folds\")\n",
    "        summary_file.write(\"\\n\")\n",
    "        summary_file.write(\n",
    "            tabulate(\n",
    "                pd.DataFrame(\n",
    "                    test_metric_result\n",
    "                ).round(4).transpose(),\n",
    "                tablefmt='psql', floatfmt=\".4f\", headers='keys'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # 4. Write outer fold results\n",
    "        summary_file.write(\"\\n\\n\")\n",
    "        summary_file.write(\"Outer Fold Best Estimators' Performance\")\n",
    "        summary_file.write(\"\\n\")\n",
    "        handler = pipeline.results_handler\n",
    "        performance_table = handler.get_performance_table()\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            summary_file.write(\n",
    "                tabulate(\n",
    "                    performance_table.loc[:, ['fold', 'best_config', 'n_train', 'n_validation']].transpose(),\n",
    "                    tablefmt='psql', headers='keys'\n",
    "                )\n",
    "            )\n",
    "            summary_file.write(\"\\n\")\n",
    "            summary_file.write(\n",
    "                tabulate(\n",
    "                    performance_table.loc[:, ['fold'] + list(metrics.keys())].round(4).transpose(),\n",
    "                    tablefmt='psql', floatfmt=\".4f\", headers='keys'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        outer_fold_performance = {}\n",
    "        config_evals = handler.get_config_evaluations()\n",
    "        for metric in metrics.keys():\n",
    "            # print(f'{metric}')\n",
    "            for i, j in enumerate(config_evals[metric]):\n",
    "                if f'{metric}_mean' in outer_fold_performance:\n",
    "                    # outer_fold_performance[f'{metric}_max'].append(np.max(j))\n",
    "                    outer_fold_performance[f'{metric}_mean'].append(np.mean(j))\n",
    "                    outer_fold_performance[f'{metric}_std'].append(np.std(j))\n",
    "                else:\n",
    "                    # outer_fold_performance[f'{metric}_max'] = [np.max(j)]\n",
    "                    outer_fold_performance[f'{metric}_mean'] = [np.mean(j)]\n",
    "                    outer_fold_performance[f'{metric}_std'] = [np.std(j)]\n",
    "        summary_file.write(\"\\n\")\n",
    "        summary_file.write(\n",
    "            tabulate(\n",
    "                pd.DataFrame(outer_fold_performance).round(4).transpose(),\n",
    "                tablefmt='psql', floatfmt=\".4f\", headers='keys'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhotonAI Optimize Monkey-patch\n",
    "#     Added random_state for Optimize for result replicability\n",
    "def prepare(self, pipeline_elements: list, maximize_metric: bool) -> None:\n",
    "    \"\"\"\n",
    "    Initializes hyperparameter search with scikit-optimize.\n",
    "\n",
    "    Assembles all hyperparameters of the list of PipelineElements\n",
    "    in order to prepare the hyperparameter space.\n",
    "    Hyperparameters can be accessed via pipe_element.hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "        pipeline_elements:\n",
    "            List of all PipelineElements to create the hyperparameter space.\n",
    "\n",
    "        maximize_metric:\n",
    "            Boolean to distinguish between score and error.\n",
    "\n",
    "    \"\"\"\n",
    "    self.start_time = None\n",
    "    self.optimizer = None\n",
    "    self.hyperparameter_list = []\n",
    "    self.maximize_metric = maximize_metric\n",
    "\n",
    "    # build skopt space\n",
    "    space = []\n",
    "    for pipe_element in pipeline_elements:\n",
    "        if pipe_element.__class__.__name__ == 'Switch':\n",
    "            error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\\n",
    "                        'element. We recommend the use of SMAC.'\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if hasattr(pipe_element, 'hyperparameters'):\n",
    "            for name, value in pipe_element.hyperparameters.items():\n",
    "                # if we only have one value we do not need to optimize\n",
    "                if isinstance(value, list) and len(value) < 2:\n",
    "                    self.constant_dictionary[name] = value[0]\n",
    "                    continue\n",
    "                if isinstance(value, PhotonCategorical) and len(value.values) < 2:\n",
    "                    self.constant_dictionary[name] = value.values[0]\n",
    "                    continue\n",
    "                skopt_param = self._convert_photonai_to_skopt_space(value, name)\n",
    "                if skopt_param is not None:\n",
    "                    space.append(skopt_param)\n",
    "\n",
    "    if self.constant_dictionary:\n",
    "        msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\\n",
    "              \"constant values. This run ignores following settings: \" + str(self.constant_dictionary.keys())\n",
    "        logger.warning(msg)\n",
    "        warnings.warn(msg)\n",
    "\n",
    "    if len(space) == 0:\n",
    "        msg = \"Did not find any hyperparameter to convert into skopt space.\"\n",
    "        logger.warning(msg)\n",
    "        warnings.warn(msg)\n",
    "    else:\n",
    "        self.optimizer = Optimizer(space,\n",
    "                                   base_estimator=self.base_estimator,\n",
    "                                   n_initial_points=self.n_initial_points,\n",
    "                                   initial_point_generator=self.initial_point_generator,\n",
    "                                   acq_func=self.acq_func,\n",
    "                                   acq_func_kwargs=self.acq_func_kwargs,\n",
    "                                   random_state=4\n",
    "                                  )\n",
    "    self.ask = self.ask_generator()\n",
    "    \n",
    "#    Monkey patched new prepare function\n",
    "SkOptOptimizer.prepare = prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Outer CV\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "plot_cv_indices(cv, X, y, ax)\n",
    "plt.rc('text') # , usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "plt.title('Outer Folds')\n",
    "# Save as file\n",
    "# plt.savefig('./blockingtimeseriessplit.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "training_folds = []\n",
    "for train, test in cv.split(X, y):\n",
    "    print(f'Train Set for Outer Fold {len(training_folds)}')\n",
    "    print(train)\n",
    "    training_folds.append(train)\n",
    "    # print(\"Test\")\n",
    "    # print(test[0], test[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner CV\n",
    "outer_fold_number = 0\n",
    "\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=False)\n",
    "plot_cv_indices(cv, X.iloc[training_folds[outer_fold_number]], y[training_folds[outer_fold_number]], ax)\n",
    "plt.rc('text') # , usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "plt.title(f'Inner Fold for Outer Fold {outer_fold_number}')\n",
    "# Save as file\n",
    "# plt.savefig('./blockingtimeseriessplit.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "inner_training_folds = []\n",
    "print(f'Train Sets for Outer Fold {outer_fold_number}')\n",
    "for train, test in cv.split(X.iloc[training_folds[outer_fold_number]], y[training_folds[outer_fold_number]]):\n",
    "    print(f'Train Set for Inner Fold {len(inner_training_folds)}')\n",
    "    print(train)\n",
    "    inner_training_folds.append(train)\n",
    "    # print(\"Test\")\n",
    "    # print(test[0], test[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_selection = Switch('estimators')\n",
    "estimator_selection += PipelineElement(\"LogisticRegression\",\n",
    "                           hyperparameters={\n",
    "                               'C': FloatRange(1, 10)\n",
    "                           }, class_weight='balanced', random_state=4)\n",
    "estimator_selection += PipelineElement(\"DecisionTreeClassifier\",\n",
    "                           hyperparameters={\n",
    "                               'min_samples_split': IntegerRange(2,30),\n",
    "                               'min_samples_leaf': IntegerRange(2,30)\n",
    "                           }, random_state=4, criterion='gini')\n",
    "estimator_selection += PipelineElement('LinearSVC',\n",
    "                            hyperparameters={\n",
    "                                'C': FloatRange(1, 25)\n",
    "                            }, class_weight='balanced', random_state=4)\n",
    "estimator_selection += PipelineElement('RandomForestClassifier', \n",
    "                            hyperparameters={\n",
    "                                'min_samples_split': IntegerRange(2,30),\n",
    "                                'max_features': ['auto', 'sqrt', 'log2']\n",
    "                            }, random_state=4, criterion='gini', bootstrap=True)\n",
    "estimator_selection += PipelineElement('GradientBoostingClassifier', \n",
    "                            hyperparameters={\n",
    "                                'loss': ['deviance', 'exponential'],\n",
    "                                'learning_rate': FloatRange(0.001, 1, 'logspace')\n",
    "                            }, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16,
     20,
     25,
     29
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_pipeline = Hyperpipe('1 - Initial Pipeline',\n",
    "                     outer_cv = StratifiedKFold(n_splits=5, shuffle=False),\n",
    "                     inner_cv = StratifiedKFold(n_splits=3, shuffle=False),\n",
    "                     use_test_set = False,\n",
    "                     metrics = list(metrics.keys()),\n",
    "                     best_config_metric='balanced_accuracy',\n",
    "                     optimizer='switch',\n",
    "                     optimizer_params={'name': 'sk_opt', 'n_configurations': 30},\n",
    "                     project_folder=f'./analysis/{dataset_type}{user}_{frequency}',\n",
    "                     cache_folder=f'./cache/{dataset_type}{user}_{frequency}/',\n",
    "                     verbosity=1,\n",
    "                     performance_constraints=[MinimumPerformanceConstraint('balanced_accuracy', 0.75, 'mean')])\n",
    "\n",
    "# Add learning algorithms to compare\n",
    "initial_pipeline += estimator_selection\n",
    "\n",
    "# Fit hyperpipe\n",
    "initial_pipeline.fit(X, y)\n",
    "\n",
    "# Show learning algorithms mean validation results\n",
    "# print(\"Comparison on learning algorithms on validation set\")\n",
    "# print(initial_pipeline.results_handler.get_mean_of_best_validation_configs_per_estimator())\n",
    "\n",
    "# # Show feature importances\n",
    "# print_feature_importances(initial_pipeline)\n",
    "\n",
    "# # View CV splits for debugging\n",
    "# for k,v in initial_pipeline.cross_validation.outer_folds.items():\n",
    "#     print(v.train_indices)\n",
    "#     print(v.test_indices)\n",
    "#     print(len(v.train_indices), len(v.test_indices))\n",
    "#     print()\n",
    "\n",
    "# Write other reports to summary file\n",
    "add_other_report_to_summary(initial_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=4).fit(X, y)\n",
    "\n",
    "feature_selection_pipeline = Hyperpipe('2 - Feature Selection Pipline',\n",
    "                     outer_cv = StratifiedKFold(n_splits=5, shuffle=False),\n",
    "                     inner_cv = StratifiedKFold(n_splits=3, shuffle=False),\n",
    "                     use_test_set = False,\n",
    "                     metrics = list(metrics.keys()),\n",
    "                     best_config_metric='balanced_accuracy',\n",
    "                     optimizer='switch',\n",
    "                     optimizer_params={'name': 'sk_opt', 'n_configurations': 30},\n",
    "                     project_folder=f'./analysis/{dataset_type}{user}_{frequency}',\n",
    "                     cache_folder=f'./cache/{dataset_type}{user}_{frequency}/',\n",
    "                     verbosity=1,\n",
    "                     performance_constraints=[MinimumPerformanceConstraint('balanced_accuracy', 0.75, 'mean')])\n",
    "\n",
    "feature_selection_pipeline += PipelineElement('RFE', \n",
    "                           hyperparameters={\n",
    "                               'n_features_to_select': IntegerRange(2, (len(columns) - 2))\n",
    "                           }, estimator=classifier)\n",
    "\n",
    "feature_selection_pipeline += estimator_selection\n",
    "\n",
    "# Fit hyperpipe\n",
    "feature_selection_pipeline.fit(X, y)\n",
    "\n",
    "# Show learning algorithms mean validation results\n",
    "# print(\"Comparison on learning algorithms on validation set\")\n",
    "# print(feature_selection_pipeline.results_handler.get_mean_of_best_validation_configs_per_estimator())\n",
    "\n",
    "# # Show feature importances\n",
    "# print_feature_importances(feature_selection_pipeline)\n",
    "\n",
    "# # View CV splits for debugging\n",
    "# for k,v in feature_selection_pipeline.cross_validation.outer_folds.items():\n",
    "#     print(v.train_indices)\n",
    "#     print(v.test_indices)\n",
    "#     print(len(v.train_indices), len(v.test_indices))\n",
    "#     print()\n",
    "\n",
    "# Write other reports to summary file\n",
    "add_other_report_to_summary(feature_selection_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalanced Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_imbalanced_pipeline = Hyperpipe('3 - Class Imbalanced Pipline',\n",
    "                     outer_cv = StratifiedKFold(n_splits=5, shuffle=False),\n",
    "                     inner_cv = StratifiedKFold(n_splits=3, shuffle=False),\n",
    "                     use_test_set = False,\n",
    "                     metrics = list(metrics.keys()),\n",
    "                     best_config_metric='balanced_accuracy',\n",
    "                     optimizer='switch',\n",
    "                     optimizer_params={'name': 'sk_opt', 'n_configurations': 30},\n",
    "                     project_folder=f'./analysis/{dataset_type}{user}_{frequency}',\n",
    "                     cache_folder=f'./cache/{dataset_type}{user}_{frequency}/',\n",
    "                     verbosity=1,\n",
    "                     performance_constraints=[MinimumPerformanceConstraint('balanced_accuracy', 0.75, 'mean')])\n",
    "\n",
    "tested_methods = Categorical(['RandomUnderSampler', 'RandomOverSampler', 'SMOTE', 'BorderlineSMOTE'])\n",
    "class_imbalanced_pipeline += PipelineElement('ImbalancedDataTransformer',\n",
    "                           hyperparameters={ 'method_name': tested_methods })\n",
    "\n",
    "class_imbalanced_pipeline += estimator_selection\n",
    "\n",
    "# Fit hyperpipe\n",
    "class_imbalanced_pipeline.fit(X, y)\n",
    "\n",
    "# Show learning algorithms mean validation results\n",
    "# print(\"Comparison on learning algorithms on validation set\")\n",
    "# print(class_imbalanced_pipeline.results_handler.get_mean_of_best_validation_configs_per_estimator())\n",
    "\n",
    "# # Show feature importances\n",
    "# print_feature_importances(class_imbalanced_pipeline)\n",
    "\n",
    "# # View CV splits for debugging\n",
    "# for k,v in class_imbalanced_pipeline.cross_validation.outer_folds.items():\n",
    "#     print(v.train_indices)\n",
    "#     print(v.test_indices)\n",
    "#     print(len(v.train_indices), len(v.test_indices))\n",
    "#     print()\n",
    "\n",
    "# Write other reports to summary file\n",
    "add_other_report_to_summary(class_imbalanced_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from photonai.base.hyperpipe import OutputSettings\n",
    "\n",
    "# Save original method\n",
    "original_update_settings = OutputSettings.update_settings\n",
    "\n",
    "def patched_update_settings(self, name, timestamp):\n",
    "    max_retries = 5\n",
    "    delay = 1  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            original_update_settings(self, name, timestamp)\n",
    "            break  # success, exit retry loop\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError on attempt {attempt+1}/{max_retries} when updating settings: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Raising the PermissionError.\")\n",
    "                raise\n",
    "\n",
    "# Apply monkey patch\n",
    "OutputSettings.update_settings = patched_update_settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalanced + Feature Selection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=4).fit(X, y)\n",
    "\n",
    "class_imbalanced_feature_selection_pipeline = Hyperpipe('4 - CI and FS Pipeline',\n",
    "                     outer_cv = StratifiedKFold(n_splits=5, shuffle=False),\n",
    "                     inner_cv = StratifiedKFold(n_splits=3, shuffle=False),\n",
    "                     use_test_set = False,\n",
    "                     metrics = list(metrics.keys()),\n",
    "                     best_config_metric='balanced_accuracy',\n",
    "                     optimizer='switch',\n",
    "                     optimizer_params={'name': 'sk_opt', 'n_configurations': 30},\n",
    "                     project_folder=f'./analysis/{dataset_type}{user}_{frequency}',\n",
    "                     cache_folder=f'./cache/{dataset_type}{user}_{frequency}/',\n",
    "                     verbosity=1,\n",
    "                     performance_constraints=[MinimumPerformanceConstraint('balanced_accuracy', 0.75, 'mean')])\n",
    "\n",
    "\n",
    "tested_methods = Categorical(['RandomUnderSampler', 'RandomOverSampler', 'SMOTE', 'BorderlineSMOTE'])\n",
    "class_imbalanced_feature_selection_pipeline += PipelineElement('ImbalancedDataTransformer',\n",
    "                           hyperparameters={ 'method_name': tested_methods })\n",
    "\n",
    "class_imbalanced_feature_selection_pipeline += PipelineElement('RFE', \n",
    "                           hyperparameters={\n",
    "                               'n_features_to_select': IntegerRange(2, (len(columns) - 2))\n",
    "                           }, estimator=classifier)\n",
    "\n",
    "class_imbalanced_feature_selection_pipeline += estimator_selection\n",
    "\n",
    "# Fit hyperpipe\n",
    "class_imbalanced_feature_selection_pipeline.fit(X, y)\n",
    "\n",
    "# Show learning algorithms mean validation results\n",
    "\n",
    "# print(\"Comparison on learning algorithms on validation set\")\n",
    "# print(class_imbalanced_feature_selection_pipeline.results_handler.get_mean_of_best_validation_configs_per_estimator())\n",
    "\n",
    "# # Show feature importances\n",
    "# print_feature_importances(class_imbalanced_feature_selection_pipeline)\n",
    "\n",
    "# # View CV splits for debugging\n",
    "# for k,v in class_imbalanced_feature_selection_pipeline.cross_validation.outer_folds.items():\n",
    "#     print(v.train_indices)\n",
    "#     print(v.test_indices)\n",
    "#     print(len(v.train_indices), len(v.test_indices))\n",
    "#     print()\n",
    "\n",
    "# Write other reports to summary file\n",
    "add_other_report_to_summary(class_imbalanced_feature_selection_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline1 = Hyperpipe('5 - Final Pipeline CI GB',\n",
    "                    outer_cv = StratifiedKFold(n_splits=5, shuffle=False),\n",
    "                    inner_cv = StratifiedKFold(n_splits=3, shuffle=False),\n",
    "                    use_test_set = True,\n",
    "                    metrics = list(metrics.keys()),\n",
    "                    best_config_metric='balanced_accuracy',\n",
    "                    optimizer='sk_opt',\n",
    "                    optimizer_params={'n_configurations': 30},\n",
    "                    project_folder=f'./analysis/{dataset_type}{user}_{frequency}',\n",
    "                    cache_folder=f'./cache/{dataset_type}{user}_{frequency}/',\n",
    "                    verbosity=1,\n",
    "                    performance_constraints=[MinimumPerformanceConstraint('balanced_accuracy', 0.75, 'mean')])\n",
    "\n",
    "tested_methods = Categorical(['RandomUnderSampler', 'RandomOverSampler', 'SMOTE', 'BorderlineSMOTE'])\n",
    "final_pipeline1 += PipelineElement('ImbalancedDataTransformer',\n",
    "                           hyperparameters={ 'method_name': tested_methods })\n",
    "\n",
    "final_pipeline1 += PipelineElement('GradientBoostingClassifier', \n",
    "                            hyperparameters={\n",
    "                                'loss': ['deviance', 'exponential'],\n",
    "                                'learning_rate': FloatRange(0.001, 1, 'logspace')\n",
    "                            }, random_state=4)\n",
    "\n",
    "# Fit hyperpipe\n",
    "final_pipeline1.fit(X, y)\n",
    "\n",
    "# Show learning algorithms mean validation results\n",
    "# print(\"Comparison on learning algorithms on validation set\")\n",
    "# print(final_pipeline1.results_handler.get_mean_of_best_validation_configs_per_estimator())\n",
    "\n",
    "# # Show feature importances\n",
    "# print_feature_importances(final_pipeline1)\n",
    "\n",
    "# # View CV splits for debugging\n",
    "# for k,v in final_pipeline1.cross_validation.outer_folds.items():\n",
    "#     print(v.train_indices)\n",
    "#     print(v.test_indices)\n",
    "#     print(len(v.train_indices), len(v.test_indices))\n",
    "#     print()\n",
    "\n",
    "# Write other reports to summary file\n",
    "add_other_report_to_summary(final_pipeline1, with_estimator_comparison=False)\n",
    "\n",
    "# Show confusion matrix\n",
    "plot_confusion_matrix_from_pipeline(final_pipeline1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03737f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 1: Built-in Feature Importances (Fastest)\n",
    "print(\"Method 1: Built-in GradientBoosting Feature Importances\")\n",
    "print(\"-\" * 55)\n",
    "try:\n",
    "    # Get the trained estimator\n",
    "    gb_estimator = final_pipeline1.optimum_pipe.named_steps['GradientBoostingClassifier']\n",
    "    \n",
    "    if hasattr(gb_estimator, 'feature_importances_'):\n",
    "        importances = gb_estimator.feature_importances_\n",
    "        feature_names = np.array(columns[1:-1])\n",
    "        \n",
    "        # Sort by importance\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"{feature_names[idx]:<20} {importances[idx]:.4f}\")\n",
    "    else:\n",
    "        print(\"Built-in feature importances not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 2: Using sklearn's permutation_importance directly\n",
    "print(\"Method 2: Sklearn Permutation Importance\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    # Get the fitted pipeline\n",
    "    fitted_pipeline = final_pipeline1.optimum_pipe\n",
    "    \n",
    "    # Use a subset of data for faster computation (optional)\n",
    "    # X_sample = X.sample(n=min(1000, len(X)), random_state=42)\n",
    "    # y_sample = y.loc[X_sample.index]\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    perm_importance = permutation_importance(\n",
    "        fitted_pipeline, X, y, \n",
    "        n_repeats=10,  # Reduced for speed\n",
    "        random_state=42,\n",
    "        scoring='balanced_accuracy'\n",
    "    )\n",
    "    \n",
    "    feature_names = np.array(columns[1:-1])\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(perm_importance.importances_mean)[::-1]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        print(f\"{feature_names[idx]:<20} {perm_importance.importances_mean[idx]:.4f} \"\n",
    "              f\"±{perm_importance.importances_std[idx]:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f056a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 3: SHAP Values (if you have shap installed)\n",
    "print(\"Method 3: SHAP Feature Importance\")\n",
    "print(\"-\" * 35)\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Get the fitted pipeline\n",
    "    fitted_pipeline = final_pipeline1.optimum_pipe\n",
    "    \n",
    "    # Use a sample for SHAP (it can be slow on large datasets)\n",
    "    X_sample = X.sample(n=min(500, len(X)), random_state=42)\n",
    "    \n",
    "    # Create explainer\n",
    "    explainer = shap.Explainer(fitted_pipeline, X_sample)\n",
    "    shap_values = explainer(X_sample)\n",
    "    \n",
    "    # Get mean absolute SHAP values as feature importance\n",
    "    feature_importance = np.abs(shap_values.values).mean(0)\n",
    "    feature_names = np.array(columns[1:-1])\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        print(f\"{feature_names[idx]:<20} {feature_importance[idx]:.4f}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcee040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 4: Simple DataFrame approach for better visualization\n",
    "print(\"Method 4: DataFrame Summary\")\n",
    "print(\"-\" * 25)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Using the original hyperpipe method but organizing results better\n",
    "    r = final_pipeline1.get_permutation_feature_importances(\n",
    "        n_repeats=20,  # Reduced for speed\n",
    "        random_state=0, \n",
    "        scoring='balanced_accuracy'\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame for better organization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': np.array(columns[1:-1]),\n",
    "        'Importance': r[\"mean\"],\n",
    "        'Std_Dev': r[\"std\"],\n",
    "        'Lower_Bound': r[\"mean\"] - 2 * r[\"std\"],\n",
    "        'Upper_Bound': r[\"mean\"] + 2 * r[\"std\"]\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Display top 10\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Show only statistically significant features\n",
    "    significant_features = importance_df[importance_df['Lower_Bound'] > 0]\n",
    "    \n",
    "    if len(significant_features) > 0:\n",
    "        print(f\"\\nStatistically Significant Features ({len(significant_features)}):\")\n",
    "        print(significant_features.to_string(index=False, float_format='%.4f'))\n",
    "    else:\n",
    "        print(\"\\nNo statistically significant features found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f40413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 5: Quick and Simple (Minimal Code)\n",
    "print(\"Method 5: Quick and Simple\")\n",
    "print(\"-\" * 25)\n",
    "try:\n",
    "    # Just get the built-in importances with minimal code\n",
    "    estimator = final_pipeline1.optimum_pipe.named_steps['GradientBoostingClassifier']\n",
    "    importances = estimator.feature_importances_\n",
    "    features = np.array(columns[1:-1])\n",
    "    \n",
    "    # Create sorted list of (importance, feature) tuples\n",
    "    sorted_features = sorted(zip(importances, features), reverse=True)\n",
    "    \n",
    "    print(\"Feature Importance Ranking:\")\n",
    "    for i, (importance, feature) in enumerate(sorted_features[:15], 1):\n",
    "        print(f\"{i:2d}. {feature:<20} {importance:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
