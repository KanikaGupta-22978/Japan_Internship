{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c536a1b-8895-4915-ac5a-3dd23426a66c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load libraries, configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e48ff-98ff-4399-a4f5-aec390ccf251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42036e21-6208-4e66-a819-b6f728669858",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67675766-0831-4bf7-b165-1c80988e4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'participant10'\n",
    "interval = '15min'\n",
    "\n",
    "columns = [ 'timestamp', 'heart_rate', 'steps', 'stress_score',\n",
    "            'awake', 'deep', 'light', 'rem', \n",
    "           'nonrem_total', 'total', 'nonrem_percentage', 'sleep_efficiency']\n",
    "\n",
    "# Include FonLog data\n",
    "# columns += ['time_from_last_drug_taken'] #, 'wo_duration']\n",
    "\n",
    "# Additional data\n",
    "columns += ['timestamp_dayofweek', 'timestamp_hour_sin', 'timestamp_hour_cos']\n",
    "\n",
    "# 'wearing_off' | 'wearing_off_post_meds' | 'wearing_off_lead60'\n",
    "target_column = 'wearing_off' \n",
    "columns.append(target_column)\n",
    "\n",
    "participant_dictionary = json.load(open(f'./data/participant_dictionary.json'))\n",
    "\n",
    "# CV splits\n",
    "if interval == '15min':\n",
    "    record_size_per_day = 96\n",
    "elif interval == '15s':\n",
    "    record_size_per_day = 5760\n",
    "elif interval == '1min':\n",
    "    record_size_per_day = 1440\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "#       BalancedSparseCategoricalAccuracy(),\n",
    "#       BalancedAccuracy()]\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "MAX_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "SHIFT = 4 # 1 = 15 min, 2 = 30 min, 4 = 1 hour\n",
    "MULTI_STEP_WIDTH = 36 # input 36 = 9 hours, input 96 = 24 hours\n",
    "USE_HOURLY = False\n",
    "SAVEFIG = False\n",
    "EXPERIMENT_NAME = 'with wearing-off'\n",
    "REMOVE_WEARING_OFF_IN_PREVIOUS_STEP = False\n",
    "\n",
    "# features to normalize\n",
    "# timestamp_dayofweek, wearing_off were not normalized\n",
    "normalize_features = ['heart_rate', 'steps', 'stress_score', 'awake', 'deep', \n",
    "                      'light', 'rem', 'nonrem_total', 'total', 'nonrem_percentage',\n",
    "                      'sleep_efficiency', 'timestamp_hour_sin', 'timestamp_hour_cos']\n",
    "def normalize_data(df, mean, std, normalize_features=normalize_features):\n",
    "    df_to_normalize = df.copy()\n",
    "    df_to_normalize.loc[:, normalize_features] = ((\n",
    "        df_to_normalize.loc[:, normalize_features] - mean\n",
    "    ) / std)\n",
    "    \n",
    "    return df_to_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32d625-e059-4947-9a2c-1d3e0c96175c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load & process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bcbc8-e9a4-4ede-b5f2-b36c6dcdfeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = pd.read_excel(f'./data/4-combined_data_{user}_{interval}.xlsx',\n",
    "#                               index_col=\"timestamp\",\n",
    "#                               usecols=columns,\n",
    "#                               engine='openpyxl')\n",
    "# # Fill missing data with 0\n",
    "# dataset.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter data based on participants' dictionary\n",
    "# dataset = dataset.loc[\n",
    "#     (dataset.index >= participant_dictionary[user]['start_date']) &\n",
    "#     (dataset.index < participant_dictionary[user]['end_date_plus_two'])\n",
    "# ]\n",
    "\n",
    "# column_indices = { name: i for i, name in enumerate(dataset.columns) }\n",
    "# df = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e51dc-11df-4403-a3af-54cea241be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = None\n",
    "for participant_number in range(1,11):\n",
    "    user = f'participant{participant_number}'\n",
    "    dataset = pd.read_excel(f'./data/4-combined_data_{user}_{interval}.xlsx',\n",
    "                                  index_col=\"timestamp\",\n",
    "                                  usecols=columns,\n",
    "                                  engine='openpyxl')\n",
    "        \n",
    "    # Fill missing data with 0\n",
    "    dataset.fillna(0, inplace=True)\n",
    "\n",
    "    # Filter data based on participants' dictionary\n",
    "    dataset = dataset.loc[\n",
    "        (dataset.index >= participant_dictionary[user]['start_date']) &\n",
    "        (dataset.index < participant_dictionary[user]['end_date_plus_two'])\n",
    "    ].assign(pid=participant_number)\n",
    "    combined_dataset = pd.concat([combined_dataset, dataset])\n",
    "\n",
    "dataset = combined_dataset\n",
    "column_indices = { name: i for i, name in enumerate(dataset.columns) }\n",
    "df = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857f17b-b1f6-4f65-99c2-34dd6233b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_excel(\"4-combined_data_15min.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468b717-0eb5-4e56-90e4-9db732cd09a9",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794dd1aa-41da-4bf4-8b8c-1ab72aaea743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data 60% \n",
    "TRAINING_PERCENTAGE = 0.6\n",
    "# validation data 20%\n",
    "VALIDATION_PERCENTAGE = 0.2\n",
    "\n",
    "column_indices = { name: i for i, name in enumerate(df.columns) }\n",
    "total_rows = len(df)\n",
    "num_features = len(df.columns)\n",
    "\n",
    "training_end_index = int(total_rows * TRAINING_PERCENTAGE)\n",
    "validation_end_index = int(total_rows * (TRAINING_PERCENTAGE + VALIDATION_PERCENTAGE))\n",
    "\n",
    "train_df = df[0:training_end_index].copy()\n",
    "val_df = df[training_end_index:validation_end_index].copy()\n",
    "test_df = df[validation_end_index:].copy()\n",
    "\n",
    "print(f\"Training data: {round(len(train_df)/record_size_per_day, 3)} days\")\n",
    "print(f\"Validation data: {round(len(val_df)/record_size_per_day, 3)} days\")\n",
    "print(f\"Test data: {round(len(test_df)/record_size_per_day, 3)} days\")\n",
    "print(f\"Total data: {round(len(df)/record_size_per_day, 3)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ca823-be9f-4e3b-80a5-50a4b3f83d84",
   "metadata": {},
   "source": [
    "# Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a0b06-d13b-4614-aa9d-fa338d9d9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean = train_df.loc[:, normalize_features].mean()\n",
    "# train_std = train_df.loc[:, normalize_features].std()\n",
    "\n",
    "# train_df = normalize_data(train_df, train_mean, train_std)\n",
    "# val_df = normalize_data(val_df, train_mean, train_std)\n",
    "# test_df = normalize_data(test_df, train_mean, train_std)\n",
    "\n",
    "# df_std = (df - train_mean) / train_std\n",
    "# df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "# _ = ax.set_xticklabels(df.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41c7ad-798c-40a5-88d7-50f3db032760",
   "metadata": {},
   "source": [
    "# WindowGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba5aeb-2030-469c-a0e8-3a8115f4821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df, val_df, test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df.reindex(columns=[x for x in list(train_df.columns) if x not in label_columns] + label_columns)\n",
    "    self.val_df = val_df.reindex(columns=[x for x in list(val_df.columns) if x not in label_columns] + label_columns)\n",
    "    self.test_df = test_df.reindex(columns=[x for x in list(test_df.columns) if x not in label_columns] + label_columns)\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(self.train_df.columns)}\n",
    "    self.input_columns = {x: self.column_indices[x] for x in\n",
    "                          self.column_indices\n",
    "                          if x not in self.label_columns}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "  def split_window(self, features):\n",
    "    inputs = features[:, self.input_slice, list(self.input_columns.values())[0]:(list(self.input_columns.values())[-1]+1)]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    if self.label_columns is not None:\n",
    "      labels = tf.stack(\n",
    "          [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "          axis=-1)\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "  def plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n",
    "    inputs, labels = self.example\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = self.column_indices[plot_col]\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    for n in range(max_n):\n",
    "      plt.subplot(max_n, 1, n+1)\n",
    "      plt.ylabel(f'{plot_col} [normed]')\n",
    "      plt.ylim(-0.1,1.1)\n",
    "      ax.set_yticks(\n",
    "          [0.0, 0.5, 1.0]\n",
    "      )\n",
    "      plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "               label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "      if self.label_columns:\n",
    "        label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "      else:\n",
    "        label_col_index = plot_col_index\n",
    "\n",
    "      if label_col_index is None:\n",
    "        continue\n",
    "\n",
    "      plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                  edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "      if model is not None:\n",
    "        predictions = model(inputs)\n",
    "        plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                    marker='X', edgecolors='k', label='Predictions',\n",
    "                    c='#ff7f0e', s=64)\n",
    "\n",
    "      if n == 0:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.xlabel('Time [h]')\n",
    "\n",
    "  def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=None,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,)\n",
    "\n",
    "    ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "  @property\n",
    "  def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "  @property\n",
    "  def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "  @property\n",
    "  def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "  @property\n",
    "  def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "      # No example batch was found, so get one from the `.train` dataset\n",
    "      result = next(iter(self.train))\n",
    "      # And cache it for next time\n",
    "      self._example = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95ffd6-28f2-4b20-80e0-46e20bf30317",
   "metadata": {},
   "source": [
    "# Compile & Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501624b5-9065-4e63-a050-255d9a868562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min',\n",
    "                                                    restore_best_weights=True)\n",
    "\n",
    "  # model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "  #               optimizer=tf.keras.optimizers.Adam(),\n",
    "  #               metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                metrics=METRICS)\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfe932-df81-4b76-84a7-75b78d4611e1",
   "metadata": {},
   "source": [
    "# Multi-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b40c1-33ec-491d-a006-43c9329b1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_WIDTH = MULTI_STEP_WIDTH\n",
    "OUT_STEPS = SHIFT * 24\n",
    "multi_window = WindowGenerator(input_width=MULTI_STEP_WIDTH,\n",
    "                               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS,\n",
    "                               label_columns=['wearing_off']\n",
    "                              )\n",
    "multi_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1a36e-6d64-4f50-8633-6c903c8f5938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "multi_conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "    # Shape => [batch, 1, conv_units]\n",
    "    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    # Shape => [batch, 1,  out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS,\n",
    "                          activation='sigmoid',\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, -1])\n",
    "\n",
    "\n",
    "    # tf.keras.layers.Conv1D(filters=64,\n",
    "    #                        kernel_size=(MULTI_STEP_WIDTH,),\n",
    "    #                        activation='relu'),\n",
    "    # tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    # tf.keras.layers.Dense(units=4, activation='sigmoid', name=\"output\"),\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_conv_model, multi_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a0e44-fe6b-45aa-a77b-a968cf865c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multi_conv_model.evaluate(multi_window.val))\n",
    "print(multi_conv_model.evaluate(multi_window.test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c4124-1ff9-4d3d-9dcd-2beb8f0e2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in zip(multi_conv_model.metrics_names, multi_conv_model.evaluate(multi_window.test, verbose=0)):\n",
    "  print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667b9b3-3a2c-471f-913b-25631fb61a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['loss'] #, 'balanced_accuracy', 'auc', 'prc', 'precision', 'recall']\n",
    "plt.figure(figsize=(25, 10))\n",
    "for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(3,3,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    # if metric == 'loss':\n",
    "    #   plt.ylim([0, plt.ylim()[1]])\n",
    "    # # elif metric == 'auc':\n",
    "    # #   plt.ylim([0.8,1])\n",
    "    # else:\n",
    "    #   plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459b634-b972-4f19-90ab-25bf7e88959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(new_df):\n",
    "    return np.array(new_df, dtype=np.float32)[np.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952253e-22d6-4bee-b9e0-6abac175a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader(train_df.loc[:, train_df.columns != 'wearing_off'].iloc[0:36]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc487e-7297-49fc-a8b2-519fa3ed6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_conv_model.predict(data_loader(train_df.loc[:, train_df.columns != 'wearing_off'].iloc[0:36])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2db08b-528f-4b6b-aa5d-8ea497640f29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95282944-06c5-4df5-b970-55e8fabf2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"models\"\n",
    "model_version = \"4\"\n",
    "model_name = \"multi_conv_model\"\n",
    "model_path = os.path.join(base_path, model_name, model_version)\n",
    "tf.saved_model.save(multi_conv_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd8ede-dbc3-457c-a610-a4bf2b9f0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.saved_model.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095914f-8b29-4922-b50a-f58091e4f630",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model(data_loader(train_df.loc[:, test_df.columns != 'wearing_off'].iloc[0:36]), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d8844-7e3e-4f91-8f60-fb97df4fd640",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model(data_loader(test_df.iloc[0:24, :]).tolist(), training=False)[:,:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe8cae-12d8-48f5-8714-0e1a08944c10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data_loader(train_df.loc[:, test_df.columns != 'wearing_off'].iloc[0:36]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44774cb7-8881-4172-8bbb-f9424aa893c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader(test_df.iloc[0:36, :]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
