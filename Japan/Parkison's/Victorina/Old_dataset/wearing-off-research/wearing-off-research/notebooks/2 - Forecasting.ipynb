{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Wearing-off\n",
    "\n",
    "References:\n",
    "* [Machine Learning Mastery's Time Series Tutorial](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/)\n",
    "* [Tensorflow's Time Series Tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from openTSNE import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# For evaluation\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# For resampling\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant to process\n",
    "USER = 'participant1'\n",
    "# USER = f'participant{sys.argv[1]}'\n",
    "\n",
    "# Collection dataset\n",
    "# COLLECTION = '2-person'\n",
    "COLLECTION = '10-person'\n",
    "# COLLECTION = '3-person'\n",
    "\n",
    "# Define base path\n",
    "BASE_DATA_PATH = '/workspaces/data'\n",
    "\n",
    "# Define results path\n",
    "RESULTS_PATH = '/workspaces/results'\n",
    "\n",
    "FIGSIZE = (20, 7)\n",
    "FIGSIZE_CM = (13, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features\n",
    "# Garmin features\n",
    "features = ['heart_rate', 'steps', 'stress_score',\n",
    "            'awake', 'deep', 'light', 'rem',\n",
    "            'nonrem_total', 'total', 'nonrem_percentage', 'sleep_efficiency']\n",
    "\n",
    "# FonLog features\n",
    "# features += ['time_from_last_drug_taken', 'wo_duration']\n",
    "\n",
    "# Additional features\n",
    "# features += ['timestamp_hour', 'timestamp_dayofweek',\n",
    "features += ['timestamp_dayofweek',\n",
    "             'timestamp_hour_sin', 'timestamp_hour_cos']\n",
    "\n",
    "TARGET_COLUMN = 'wearing_off'\n",
    "features.append(TARGET_COLUMN)\n",
    "\n",
    "columns = ['timestamp'] + features + ['participant']\n",
    "\n",
    "# Normalize features\n",
    "normalize_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics & Other Hyperparameters\n",
    "SHIFT = 4\n",
    "RECORD_SIZE_PER_DAY = 96  # 60 minutes / 15 minutes * 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set periods\n",
    "test_set_horizons = {\n",
    "  \"participant1\": [\"2021-12-02 0:00\", \"2021-12-03 23:45\"],\n",
    "  \"participant2\": [\"2021-11-28 0:00\", \"2021-11-29 23:45\"],\n",
    "  \"participant3\": [\"2021-11-25 0:00\", \"2021-11-26 23:45\"],\n",
    "  \"participant4\": [\"2021-12-06 0:00\", \"2021-12-07 7:15\"],\n",
    "  \"participant5\": [\"2021-11-28 0:00\", \"2021-11-29 23:45\"],\n",
    "  \"participant6\": [\"2021-12-06 0:00\", \"2021-12-07 23:45\"],\n",
    "  \"participant7\": [\"2021-12-12 0:00\", \"2021-12-13 9:45\"],\n",
    "  \"participant8\": [\"2021-12-23 0:00\", \"2021-12-24 23:45\"],\n",
    "  \"participant9\": [\"2021-12-23 0:00\", \"2021-12-24 23:45\"],\n",
    "  \"participant10\": [\"2021-12-23 0:00\", \"2021-12-24 23:45\"],\n",
    "  \"participant11\": [\"2023-01-30 0:00\", \"2023-01-31 23:45\"],\n",
    "  \"participant12\": [\"2023-01-10 0:00\", \"2023-01-11 23:45\"],\n",
    "  \"participant13\": [\"2023-01-29 0:00\", \"2023-01-30 23:45\"],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant's Excel file\n",
    "dataset = pd.read_excel(f'{BASE_DATA_PATH}/{COLLECTION}/combined_data.xlsx',\n",
    "                        index_col=\"timestamp\",\n",
    "                        usecols=columns,\n",
    "                        engine='openpyxl')\n",
    "\n",
    "# Fill missing data with 0\n",
    "dataset.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant's Excel file\n",
    "dataset2 = pd.read_excel(f'{BASE_DATA_PATH}/3-person/combined_data.xlsx',\n",
    "                         index_col=\"timestamp\",\n",
    "                         usecols=columns,\n",
    "                         engine='openpyxl')\n",
    "\n",
    "# Fill missing data with 0\n",
    "dataset2.fillna(0, inplace=True)\n",
    "\n",
    "# Combine datasets\n",
    "dataset = pd.concat([dataset, dataset2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration for visualization\n",
    "\n",
    "# Font size\n",
    "# xx-large: 17.28\n",
    "# Reference: https://stackoverflow.com/a/62289322/2303766\n",
    "plt.rcParams.update({'font.size': '17.28'})\n",
    "\n",
    "# increase plot fontsize by 20% of default size\n",
    "# plt.rcParams.update({'font.size': 1.2 * plt.rcParams['font.size']})\n",
    "\n",
    "# reset fontsize to default\n",
    "# plt.rcParams.update({'font.size': plt.rcParamsDefault['font.size']})\n",
    "\n",
    "########################################\n",
    "# Garmin features\n",
    "features_to_visualize = ['heart_rate', 'steps', 'stress_score',\n",
    "                         'awake', 'deep', 'light', 'rem',\n",
    "                         'nonrem_total', 'total', 'nonrem_percentage', 'sleep_efficiency']\n",
    "\n",
    "# FonLog features\n",
    "# features_to_visualize += ['time_from_last_drug_taken', 'wo_duration']\n",
    "\n",
    "# Additional features\n",
    "# features_to_visualize += ['timestamp_hour', 'timestamp_dayofweek',\n",
    "features_to_visualize += ['timestamp_dayofweek',\n",
    "                          'timestamp_hour_sin', 'timestamp_hour_cos']\n",
    "\n",
    "########################################\n",
    "# Figure size\n",
    "FIGSIZE_VIZ = (20, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wearing-off distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Wearing-off Distribution\n",
    "# normalized value_count per row per participant\n",
    "imbalance_table = dataset.pivot_table(\n",
    "  index='participant',\n",
    "  columns='wearing_off',\n",
    "  aggfunc='size'\n",
    ").apply(lambda x: x / x.sum() * 100, axis=1).round(3)\n",
    "\n",
    "imbalance_table[[1]].plot(kind='hist', figsize=FIGSIZE_VIZ,\n",
    "                          title='Wearing-off Percentage Distribution')\n",
    "\n",
    "# add dashed line at the median\n",
    "plt.axvline(imbalance_table[1].median(), color='k',\n",
    "            linestyle='dashed', linewidth=1)\n",
    "# add label to the dashed line\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(imbalance_table[1].median() * 0.72, max_ylim * 0.9,\n",
    "         'Median Wearing-off %: {:.2f}%'.format(imbalance_table[1].median()))\n",
    "\n",
    "# add dashed line at the mean\n",
    "plt.axvline(imbalance_table[1].mean(), color='r',\n",
    "            linestyle='dashed', linewidth=1)\n",
    "# add label to the dashed line\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(imbalance_table[1].mean() * 1.025, max_ylim * 0.8,\n",
    "         'Mean Wearing-off %: {:.2f}%'.format(imbalance_table[1].mean()),\n",
    "         color='r')\n",
    "\n",
    "# add y-axis label\n",
    "plt.ylabel('Number of Participants')\n",
    "\n",
    "# add x-axis label\n",
    "plt.xlabel('Wearing-off %')\n",
    "\n",
    "# Remvoe legend\n",
    "plt.legend().remove()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "dataset.pivot_table(\n",
    "  index='participant',\n",
    "  columns='wearing_off',\n",
    "  aggfunc='size'\n",
    ").apply(lambda x: x / x.sum() * 100, axis=1).round(3).sort_values(by=1, ascending=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series of features with wearing-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Features with wearing-off across time\n",
    "axes = dataset.loc[:, features_to_visualize].plot(\n",
    "  subplots=True, figsize=FIGSIZE)\n",
    "for index, axis in enumerate(axes):\n",
    "  if index == 0:\n",
    "    axis.set_title(\n",
    "      label='Garmin data with wearing-off periods for Participant {}'.format(\n",
    "        USER.replace(\"participant\", \"\")\n",
    "      ),\n",
    "      fontdict={\n",
    "        'fontsize': 'xx-large',\n",
    "        'fontweight': 'bold'\n",
    "      }\n",
    "    )\n",
    "\n",
    "  column = dataset.columns[index]\n",
    "  min_value = dataset[[column]].min()\n",
    "  max_value = dataset[[column]].max()\n",
    "\n",
    "  if min_value.values[0] > 0:\n",
    "    min_value = 0\n",
    "\n",
    "  axis.fill_between(\n",
    "      x=dataset.index, y1=min_value, y2=max_value,\n",
    "      where=dataset.wearing_off,\n",
    "      alpha=0.4, color=\"red\", transform=axis.get_xaxis_transform()\n",
    "  )\n",
    "  axis.legend([column], loc=\"lower right\", fontsize='xx-large')\n",
    "  axis.tick_params(axis='both', which='both', labelsize='xx-large')\n",
    "  axis.set_xlabel(xlabel=dataset.index.name, fontsize='xx-large')\n",
    "\n",
    "  # Add vertical line at test_horizon[0]\n",
    "  axis.axvline(x=test_set_horizons[USER][0], color='black', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wearing-off heatmap per day, per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Heatmap of wearing-off per day per hour\n",
    "pivot_table = dataset.pivot_table(index=dataset.index.date,\n",
    "                                  columns=dataset.index.hour,\n",
    "                                  values='wearing_off',\n",
    "                                  aggfunc='sum'\n",
    "                                  ).fillna(0).astype(int)\n",
    "\n",
    "# plot heatmap of wearing-off per day per hour, and\n",
    "#   text color matches the background color\n",
    "pivot_table.style.background_gradient(\n",
    "  cmap='Reds', axis=None\n",
    ").text_gradient(\n",
    "  cmap='Reds', axis=None\n",
    ")\n",
    "\n",
    "# Display pivot_table as figure\n",
    "plt.figure(figsize=(20, 20))\n",
    "# Add label to sns's colorbar\n",
    "sns.heatmap(pivot_table, cmap='Reds', annot=False, fmt='d',\n",
    "            cbar_kws={'label': 'Count of Wearing-off'})\n",
    "plt.title('Wearing-off per day per hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Day')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (PCA) Boundary between wearing-off with non-wearing-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Compress all columns into 2 columns using PCA\n",
    "pca = PCA(n_components=2, random_state=4)\n",
    "\n",
    "# Fit PCA\n",
    "pca.fit(dataset[features_to_visualize])\n",
    "\n",
    "# Transform dataset\n",
    "dataset_pca = pca.transform(dataset[features_to_visualize])\n",
    "\n",
    "# Create dataframe from dataset_pca and dataset.wearing_off\n",
    "dataset_pca = pd.DataFrame(dataset_pca, columns=['pca1', 'pca2'])\n",
    "dataset_pca['wearing_off'] = dataset.wearing_off.values\n",
    "\n",
    "# Plot PCA with line that separates wearing-off (1) and non-wearing-off (0)\n",
    "plt.figure(figsize=FIGSIZE_VIZ)\n",
    "\n",
    "plt.scatter(dataset_pca[dataset_pca.wearing_off == 0].pca1,\n",
    "            dataset_pca[dataset_pca.wearing_off == 0].pca2,\n",
    "            color='blue', alpha=0.5, label='Non-wearing-off')\n",
    "\n",
    "plt.scatter(dataset_pca[dataset_pca.wearing_off == 1].pca1,\n",
    "            dataset_pca[dataset_pca.wearing_off == 1].pca2,\n",
    "            color='red', alpha=0.5, label='Wearing-off')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Wearing-off and Non-wearing-off using PCA (n=2)')\n",
    "\n",
    "# Add dashed lines at 0,0\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Compress all columns into 3 columns using PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3, random_state=4)\n",
    "\n",
    "# Fit PCA\n",
    "pca.fit(dataset[features_to_visualize])\n",
    "\n",
    "# Transform dataset\n",
    "dataset_pca = pca.transform(dataset[features_to_visualize])\n",
    "\n",
    "# Create dataframe from dataset_pca and dataset.wearing_off\n",
    "dataset_pca = pd.DataFrame(dataset_pca, columns=['pca1', 'pca2', 'pca3'])\n",
    "dataset_pca['wearing_off'] = dataset.wearing_off.values\n",
    "\n",
    "# Plot PCA with line that separates wearing-off (1) and non-wearing-off (0)\n",
    "fig = plt.figure(figsize=FIGSIZE_VIZ)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(dataset_pca[dataset_pca.wearing_off == 0].pca1,\n",
    "           dataset_pca[dataset_pca.wearing_off == 0].pca2,\n",
    "           dataset_pca[dataset_pca.wearing_off == 0].pca3,\n",
    "           color='blue', alpha=0.5, label='Non-wearing-off')\n",
    "\n",
    "ax.scatter(dataset_pca[dataset_pca.wearing_off == 1].pca1,\n",
    "           dataset_pca[dataset_pca.wearing_off == 1].pca2,\n",
    "           dataset_pca[dataset_pca.wearing_off == 1].pca3,\n",
    "           color='red', alpha=0.5, label='Wearing-off')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel('PCA 3')\n",
    "ax.set_title('Wearing-off and Non-wearing-off using PCA (n=3)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (t-SNE) Boundary between wearing-off and non-wearing-off using tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Compress all columns into 2 columns using t-SNE\n",
    "tsne = TSNE(\n",
    "  metric=\"euclidean\",\n",
    "  n_jobs=-1,\n",
    "  random_state=4,\n",
    "  verbose=True,\n",
    ")\n",
    "\n",
    "# Fit t-SNE\n",
    "dataset_tsne = tsne.fit(dataset[features_to_visualize].values)\n",
    "\n",
    "# Create dataframe from dataset_tsne and dataset.wearing_off\n",
    "dataset_tsne = pd.DataFrame(dataset_tsne, columns=['tsne1', 'tsne2'])\n",
    "dataset_tsne['wearing_off'] = dataset.wearing_off.values\n",
    "\n",
    "# Plot t-SNE with line that separates wearing-off (1) and non-wearing-off (0)\n",
    "plt.figure(figsize=FIGSIZE_VIZ)\n",
    "\n",
    "plt.scatter(dataset_tsne[dataset_tsne.wearing_off == 0].tsne1,\n",
    "            dataset_tsne[dataset_tsne.wearing_off == 0].tsne2,\n",
    "            color='blue', alpha=0.5, label='Non-wearing-off')\n",
    "\n",
    "plt.scatter(dataset_tsne[dataset_tsne.wearing_off == 1].tsne1,\n",
    "            dataset_tsne[dataset_tsne.wearing_off == 1].tsne2,\n",
    "            color='red', alpha=0.5, label='Wearing-off')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "\n",
    "plt.title('Wearing-off and Non-wearing-off using t-SNE')\n",
    "\n",
    "# Add dashed lines at 0,0\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (UMAP) Boundary between wearing-off and non-wearing-off using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Compress all columns into 2 columns using UMAP\n",
    "# Fit UMAP\n",
    "umap_model = umap.UMAP(n_components=2, random_state=4)\n",
    "umap_model.fit(dataset[features_to_visualize])\n",
    "\n",
    "# Transform dataset\n",
    "dataset_umap = umap_model.transform(dataset[features_to_visualize])\n",
    "dataset_umap = pd.DataFrame(dataset_umap, columns=['umap1', 'umap2'])\n",
    "dataset_umap['wearing_off'] = dataset.wearing_off.values\n",
    "\n",
    "# Plot UMAP with line that separates wearing-off (1) and non-wearing-off (0)\n",
    "plt.figure(figsize=FIGSIZE_VIZ)\n",
    "\n",
    "plt.scatter(dataset_umap[dataset_umap.wearing_off == 0].umap1,\n",
    "            dataset_umap[dataset_umap.wearing_off == 0].umap2,\n",
    "            color='blue', alpha=0.5, label='Non-wearing-off')\n",
    "\n",
    "plt.scatter(dataset_umap[dataset_umap.wearing_off == 1].umap1,\n",
    "            dataset_umap[dataset_umap.wearing_off == 1].umap2,\n",
    "            color='red', alpha=0.5, label='Wearing-off')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "\n",
    "plt.title('Wearing-off and Non-wearing-off using UMAP')\n",
    "\n",
    "# Add dashed lines at 0,0\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a copy of the original dataset\n",
    "original_dataset = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'participant13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by participant\n",
    "dataset = original_dataset.query(f'participant == \"{USER}\"').drop(\n",
    "    columns=['participant']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset.loc[\n",
    "  dataset.index < test_set_horizons[USER][0]\n",
    "].copy()\n",
    "test_df = dataset.loc[test_set_horizons[USER][0]:].copy()\n",
    "\n",
    "# # Divide train_df to train_df and validation_df where validation_df is the last 20% of train_df\n",
    "# validation_df = train_df.iloc[int(len(train_df) * 0.8):].copy()\n",
    "# train_df = train_df.iloc[:int(len(train_df) * 0.8)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # OVERRIDE\n",
    "train_df = dataset.copy()\n",
    "test_df = dataset.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Visualize train and test split for each participant\n",
    "\n",
    "for user in test_set_horizons.keys():\n",
    "  # Filter by participant\n",
    "  dataset = original_dataset.query(f'participant == \"{user}\"').drop(\n",
    "      columns=['participant']).copy()\n",
    "\n",
    "  ################################\n",
    "  train_df = dataset.loc[\n",
    "    dataset.index < test_set_horizons[user][0]\n",
    "  ].copy()\n",
    "  test_df = dataset.loc[test_set_horizons[user][0]:].copy()\n",
    "\n",
    "  # Plot wearing-off periods from train and test set within 2, 1 subplot\n",
    "  fig, axes = plt.subplots(2, 1, figsize=FIGSIZE_VIZ)\n",
    "\n",
    "  # Plot train wearing-off periods\n",
    "  axes[0].plot(train_df.wearing_off,\n",
    "               label='train set', color='blue',)  # marker='o',)\n",
    "\n",
    "  # Dashed horizontal line at 0.5\n",
    "  axes[0].axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "  # Dashed vertical lines on each start of the day\n",
    "  for i in train_df.index:\n",
    "    if pd.Timestamp(i).hour == 0 and pd.Timestamp(i).minute == 0:\n",
    "      axes[0].axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "  # Set x and y labels\n",
    "  # axes[0].set_xlabel('Time')\n",
    "  # axes[0].set_ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "  # Set title\n",
    "  axes[0].set_title(\n",
    "    f'Wearing-off Periods for {user.upper()} (Train set with {round(len(train_df) / RECORD_SIZE_PER_DAY, 1)} days))')\n",
    "\n",
    "  ################################\n",
    "  # Plot test wearing-off periods\n",
    "  axes[1].plot(test_df.wearing_off,\n",
    "               label='test set', color='blue',)  # marker='o',)\n",
    "\n",
    "  # Dashed horizontal line at 0.5\n",
    "  axes[1].axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "  # Dashed vertical lines on each start of the day\n",
    "  for i in test_df.index:\n",
    "    if pd.Timestamp(i).hour == 0 and pd.Timestamp(i).minute == 0:\n",
    "      axes[1].axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "  # Set x and y labels\n",
    "  axes[1].set_xlabel('Time')\n",
    "  # axes[1].set_ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "  # Set title\n",
    "  axes[1].set_title(\n",
    "    f'Wearing-off Periods for {user.upper()} (Test set with {round(len(test_df) / RECORD_SIZE_PER_DAY, 1)} days))')\n",
    "\n",
    "  # Add 1 y-axis label for figure\n",
    "  fig.text(0.08, 0.5, 'Wearing-off Forecast Probability',\n",
    "           va='center', rotation='vertical')\n",
    "\n",
    "  # Show figure\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform series to supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "  var_names = data.columns\n",
    "  n_vars = len(var_names)\n",
    "  df = pd.DataFrame(data)\n",
    "  cols, names = list(), list()  # new column values, new columne names\n",
    "\n",
    "  # input sequence (t-i, ... t-1)\n",
    "  # timesteps before (e.g., n_in = 3, t-3, t-2, t-1)\n",
    "  for i in range(n_in, 0, -1):\n",
    "    cols.append(df.shift(i))\n",
    "    names += list(\n",
    "        map(lambda var_name: f'{var_name}(t-{i})', var_names)\n",
    "    )\n",
    "\n",
    "  # forecast sequence (t, t+1, ... t+n)\n",
    "  # timesteps after (e.g., n_out = 3, t, t+1, t+2)\n",
    "  for i in range(0, n_out):\n",
    "    cols.append(df.shift(-i))\n",
    "    if i == 0:\n",
    "      names += list(map(lambda var_name: f'{var_name}(t)', var_names))\n",
    "    else:\n",
    "      names += list(map(lambda var_name: f'{var_name}(t+{i})', var_names))\n",
    "\n",
    "  # put it all together\n",
    "  agg = pd.concat(cols, axis=1)\n",
    "  agg.columns = names\n",
    "\n",
    "  # drop rows with NaN values\n",
    "  if dropnan:\n",
    "    agg.dropna(inplace=True)\n",
    "\n",
    "  return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "def split_x_y(df, target_columns, SHIFT=SHIFT):\n",
    "  # Drop extra columns i.e., (t+1), (t+2), (t+3), (t+4)\n",
    "  regex = r\".*\\(t\\+[1-{SHIFT}]\\)$\"  # includes data(t)\n",
    "  # regex = r\"\\(t(\\+([1-{SHIFT}]))?\\)$\" # removes data(t)\n",
    "\n",
    "  # Drop extra columns except target_columns\n",
    "  df.drop(\n",
    "    [x for x in df.columns if re.search(regex, x) and x not in target_columns],\n",
    "    axis=1, inplace=True\n",
    "  )\n",
    "\n",
    "  # Split into X and y\n",
    "  y = df[target_columns].copy()\n",
    "  X = df.drop(target_columns, axis=1)\n",
    "\n",
    "  return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IN = SHIFT  # Last hour\n",
    "N_OUT = SHIFT + 1  # Next hour\n",
    "\n",
    "# For a similar sliding window with TF's WindowGenerator,\n",
    "#   n_in = last day, t - 47\n",
    "#   n_out = next 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train set to supervised learning\n",
    "reframed_train_df = series_to_supervised(train_df,\n",
    "                                         n_in=N_IN,\n",
    "                                         n_out=N_OUT,\n",
    "                                         dropnan=True)\n",
    "\n",
    "train_X, train_y = split_x_y(\n",
    "    reframed_train_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "\n",
    "# display(train_y.head(5))\n",
    "# display(train_X.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with validation data's N_IN tail\\nreframed_test_df = series_to_supervised(pd.concat([validation_df.tail(N_IN),\\n                                                   test_df,\\n                                                   ]),\\n                                        n_in=N_IN,\\n                                        n_out=N_OUT,\\n                                        dropnan=True)\\ntest_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert test set to supervised learning\n",
    "# with train data's N_IN tail\n",
    "reframed_test_df = series_to_supervised(pd.concat([train_df.tail(N_IN),\n",
    "                                                   test_df,\n",
    "                                                   ]),\n",
    "                                        n_in=N_IN,\n",
    "                                        n_out=N_OUT,\n",
    "                                        dropnan=True)\n",
    "test_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "\n",
    "'''test data only\n",
    "reframed_test_df = series_to_supervised(test_df,\n",
    "                                        n_in=N_IN,\n",
    "                                        n_out=N_OUT,\n",
    "                                        dropnan=True)\n",
    "test_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{SHIFT})'])\n",
    "'''\n",
    "\n",
    "'''with validation data's N_IN tail\n",
    "reframed_test_df = series_to_supervised(pd.concat([validation_df.tail(N_IN),\n",
    "                                                   test_df,\n",
    "                                                   ]),\n",
    "                                        n_in=N_IN,\n",
    "                                        n_out=N_OUT,\n",
    "                                        dropnan=True)\n",
    "test_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "'''\n",
    "\n",
    "# display(test_y.head(5))\n",
    "# display(test_X.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "train_X_scaled = pd.DataFrame(train_X_scaled,\n",
    "                              columns=train_X.columns,\n",
    "                              index=train_X.index)\n",
    "test_X_scaled = scaler.fit_transform(test_X)\n",
    "test_X_scaled = pd.DataFrame(test_X_scaled,\n",
    "                             columns=test_X.columns,\n",
    "                             index=test_X.index)\n",
    "\n",
    "# display(test_X_scaled.head(5))\n",
    "# display(train_X_scaled.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "normalizer = Normalizer()\n",
    "train_X_scaled_normalized = normalizer.fit_transform(train_X_scaled)\n",
    "\n",
    "train_X_scaled_normalized = pd.DataFrame(train_X_scaled_normalized,\n",
    "                                         columns=train_X.columns,\n",
    "                                         index=train_X.index)\n",
    "\n",
    "test_X_scaled_normalized = normalizer.fit_transform(test_X_scaled)\n",
    "\n",
    "test_X_scaled_normalized = pd.DataFrame(test_X_scaled_normalized,\n",
    "                                        columns=test_X.columns,\n",
    "                                        index=test_X.index)\n",
    "\n",
    "# display(train_X_scaled_normalized.head(5))\n",
    "# display(test_X_scaled_normalized.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Override\n",
    "train_X, train_y = split_x_y(train_df, [f'{TARGET_COLUMN}'])\n",
    "test_X, test_y = split_x_y(test_df, [f'{TARGET_COLUMN}'])\n",
    "\n",
    "train_X_scaled = train_X\n",
    "test_X_scaled = test_X\n",
    "\n",
    "train_X_scaled_normalized = train_X\n",
    "test_X_scaled_normalized = test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep original data\n",
    "original_train_X_scaled_normalized = train_X_scaled_normalized.copy()\n",
    "original_train_y = train_y.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heart_rate(t-4)</th>\n",
       "      <th>steps(t-4)</th>\n",
       "      <th>stress_score(t-4)</th>\n",
       "      <th>awake(t-4)</th>\n",
       "      <th>deep(t-4)</th>\n",
       "      <th>light(t-4)</th>\n",
       "      <th>rem(t-4)</th>\n",
       "      <th>nonrem_total(t-4)</th>\n",
       "      <th>total(t-4)</th>\n",
       "      <th>nonrem_percentage(t-4)</th>\n",
       "      <th>...</th>\n",
       "      <th>light(t)</th>\n",
       "      <th>rem(t)</th>\n",
       "      <th>nonrem_total(t)</th>\n",
       "      <th>total(t)</th>\n",
       "      <th>nonrem_percentage(t)</th>\n",
       "      <th>sleep_efficiency(t)</th>\n",
       "      <th>timestamp_dayofweek(t)</th>\n",
       "      <th>timestamp_hour_sin(t)</th>\n",
       "      <th>timestamp_hour_cos(t)</th>\n",
       "      <th>wearing_off(t)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-26 14:00:00</th>\n",
       "      <td>0.166881</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.121273</td>\n",
       "      <td>0.025559</td>\n",
       "      <td>0.106392</td>\n",
       "      <td>0.128389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139846</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>0.204471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139846</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>0.204471</td>\n",
       "      <td>0.174685</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.051118</td>\n",
       "      <td>0.013697</td>\n",
       "      <td>0.204471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02 01:15:00</th>\n",
       "      <td>0.130889</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.056182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.052505</td>\n",
       "      <td>0.046949</td>\n",
       "      <td>0.145107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.052505</td>\n",
       "      <td>0.046949</td>\n",
       "      <td>0.145107</td>\n",
       "      <td>0.198694</td>\n",
       "      <td>0.132463</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.193422</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-04 11:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.032094</td>\n",
       "      <td>0.066424</td>\n",
       "      <td>0.169829</td>\n",
       "      <td>0.120055</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.186410</td>\n",
       "      <td>0.086689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169829</td>\n",
       "      <td>0.120055</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.186410</td>\n",
       "      <td>0.086689</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>0.201731</td>\n",
       "      <td>0.126972</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-17 23:15:00</th>\n",
       "      <td>0.161463</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125233</td>\n",
       "      <td>0.142749</td>\n",
       "      <td>0.118154</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>0.186788</td>\n",
       "      <td>0.158415</td>\n",
       "      <td>0.148756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118154</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>0.186788</td>\n",
       "      <td>0.158415</td>\n",
       "      <td>0.148756</td>\n",
       "      <td>0.081353</td>\n",
       "      <td>0.031131</td>\n",
       "      <td>0.075174</td>\n",
       "      <td>0.184993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-25 05:15:00</th>\n",
       "      <td>0.106058</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.038754</td>\n",
       "      <td>0.057676</td>\n",
       "      <td>0.128214</td>\n",
       "      <td>0.100330</td>\n",
       "      <td>0.023874</td>\n",
       "      <td>0.152152</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.155055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100330</td>\n",
       "      <td>0.023874</td>\n",
       "      <td>0.152152</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.155055</td>\n",
       "      <td>0.124703</td>\n",
       "      <td>0.181268</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.108316</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     heart_rate(t-4)  steps(t-4)  stress_score(t-4)  \\\n",
       "timestamp                                                             \n",
       "2023-01-26 14:00:00         0.166881    0.006711           0.121273   \n",
       "2022-12-02 01:15:00         0.130889    0.000133           0.056182   \n",
       "2022-12-04 11:00:00         0.000000    0.000135           0.002319   \n",
       "2023-01-17 23:15:00         0.161463    0.021394           0.000000   \n",
       "2022-12-25 05:15:00         0.106058    0.000121           0.038754   \n",
       "\n",
       "                     awake(t-4)  deep(t-4)  light(t-4)  rem(t-4)  \\\n",
       "timestamp                                                          \n",
       "2023-01-26 14:00:00    0.025559   0.106392    0.128389  0.000000   \n",
       "2022-12-02 01:15:00    0.000000   0.147001    0.000000  0.037800   \n",
       "2022-12-04 11:00:00    0.032094   0.066424    0.169829  0.120055   \n",
       "2023-01-17 23:15:00    0.125233   0.142749    0.118154  0.038269   \n",
       "2022-12-25 05:15:00    0.057676   0.128214    0.100330  0.023874   \n",
       "\n",
       "                     nonrem_total(t-4)  total(t-4)  nonrem_percentage(t-4)  \\\n",
       "timestamp                                                                    \n",
       "2023-01-26 14:00:00           0.139846    0.084549                0.204471   \n",
       "2022-12-02 01:15:00           0.052505    0.046949                0.145107   \n",
       "2022-12-04 11:00:00           0.136926    0.186410                0.086689   \n",
       "2023-01-17 23:15:00           0.186788    0.158415                0.148756   \n",
       "2022-12-25 05:15:00           0.152152    0.118551                0.155055   \n",
       "\n",
       "                     ...  light(t)    rem(t)  nonrem_total(t)  total(t)  \\\n",
       "timestamp            ...                                                  \n",
       "2023-01-26 14:00:00  ...  0.128389  0.000000         0.139846  0.084549   \n",
       "2022-12-02 01:15:00  ...  0.000000  0.037800         0.052505  0.046949   \n",
       "2022-12-04 11:00:00  ...  0.169829  0.120055         0.136926  0.186410   \n",
       "2023-01-17 23:15:00  ...  0.118154  0.038269         0.186788  0.158415   \n",
       "2022-12-25 05:15:00  ...  0.100330  0.023874         0.152152  0.118551   \n",
       "\n",
       "                     nonrem_percentage(t)  sleep_efficiency(t)  \\\n",
       "timestamp                                                        \n",
       "2023-01-26 14:00:00              0.204471             0.174685   \n",
       "2022-12-02 01:15:00              0.145107             0.198694   \n",
       "2022-12-04 11:00:00              0.086689             0.173300   \n",
       "2023-01-17 23:15:00              0.148756             0.081353   \n",
       "2022-12-25 05:15:00              0.155055             0.124703   \n",
       "\n",
       "                     timestamp_dayofweek(t)  timestamp_hour_sin(t)  \\\n",
       "timestamp                                                            \n",
       "2023-01-26 14:00:00                0.102236               0.051118   \n",
       "2022-12-02 01:15:00                0.132463               0.131281   \n",
       "2022-12-04 11:00:00                0.201731               0.126972   \n",
       "2023-01-17 23:15:00                0.031131               0.075174   \n",
       "2022-12-25 05:15:00                0.181268               0.179526   \n",
       "\n",
       "                     timestamp_hour_cos(t)  wearing_off(t)  \n",
       "timestamp                                                   \n",
       "2023-01-26 14:00:00               0.013697        0.204471  \n",
       "2022-12-02 01:15:00               0.193422        0.000000  \n",
       "2022-12-04 11:00:00               0.003437        0.000000  \n",
       "2023-01-17 23:15:00               0.184993        0.000000  \n",
       "2022-12-25 05:15:00               0.108316        0.000000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle train dataset\n",
    "# Combine train_y to train_X_scaled_normalized\n",
    "train = pd.concat(\n",
    "    [original_train_X_scaled_normalized, original_train_y], axis=1)\n",
    "\n",
    "# Shuffle train\n",
    "train = train.sample(frac=1, random_state=4)\n",
    "\n",
    "# Split train into X and y\n",
    "train_y = train[f'{TARGET_COLUMN}(t+{SHIFT})']\n",
    "train_X_scaled_normalized = train.drop(f'{TARGET_COLUMN}(t+{SHIFT})', axis=1)\n",
    "\n",
    "# Original train_X, train_y\n",
    "original_train_X = train_X.copy()\n",
    "original_test_X = test_X.copy()\n",
    "\n",
    "# Renamed to train_X, train_y for easier reference\n",
    "train_X = train_X_scaled_normalized.copy()\n",
    "test_X = test_X_scaled_normalized.copy()\n",
    "\n",
    "train_X.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from scipy.special import expit as sigmoid\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def logistic_obj(labels: np.ndarray, predt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "  '''\n",
    "  Logistic loss objective function for binary-class classification\n",
    "  '''\n",
    "  # grad = grad.flatten()\n",
    "  # hess = hess.flatten()\n",
    "  # return grad, hess\n",
    "  y = labels\n",
    "  p = sigmoid(predt)\n",
    "  grad = p - y\n",
    "  hess = p * (1.0 - p)\n",
    "\n",
    "  return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_class(conditional_probability, wrongprob, trueprob):\n",
    "  a = conditional_probability / (wrongprob / trueprob)\n",
    "  comp_cond = 1 - conditional_probability\n",
    "  comp_wrong = 1 - wrongprob\n",
    "  comp_true = 1 - trueprob\n",
    "  b = comp_cond / (comp_wrong / comp_true)\n",
    "  return a / (a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "if not os.path.isfile(f'{RESULTS_PATH}/metric scores.xlsx'):\n",
    "  # Create ExcelWriter\n",
    "  writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                          engine='openpyxl', mode='w')\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Metric Scores\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model',\n",
    "             'f1 score', 'recall', 'precision', 'accuracy',\n",
    "             'auc-roc', 'auc-prc']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Metric Scores', index=False)\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Classification Report\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model', 'classification report',\n",
    "             'precision', 'recall', 'f1-score', 'support']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Classification Report', index=False)\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Confusion Matrix\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Confusion Matrix', index=False)\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Sampling Ratio\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model', 'original_N0', 'original_N1', 'resampled_N0',\n",
    "             'resampled_N1', 'sampling_rate']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Sampling Ratio', index=False)\n",
    "\n",
    "  writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Base Model\n",
    "model_name = 'Base Model'\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "base_model = XGBClassifier(objective=logistic_obj,\n",
    "                           random_state=4, n_estimators=1000)\n",
    "# fit model\n",
    "base_model.fit(train_X, train_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "base_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')\n",
    "\n",
    "'''others\n",
    "# create XGBClassifier model instance\n",
    "base_model = XGBClassifier(random_state=4, n_estimators=1000)\n",
    "# Change the objective to 'binary:logitraw'\n",
    "# base_model.set_params(objective='binary:logitraw')\n",
    "# fit model\n",
    "base_model.fit(train_X, train_y)\n",
    "\n",
    "# create LogisticRegression model instance\n",
    "base_model = LogisticRegression(\n",
    "    random_state=4, max_iter=10000, penalty='l2', C=1)\n",
    "# fit model\n",
    "base_model.fit(train_X_scaled_normalized, train_y.values.ravel())\n",
    "\n",
    "# create GradientBoosting model instance\n",
    "base_model = GradientBoostingClassifier(random_state=4)\n",
    "# fit model\n",
    "base_model.fit(train_X_scaled_normalized, train_y.values.ravel())\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "update"
    ]
   },
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = base_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = base_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth', 'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")\n",
    "# forecasts_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Oversampled SMOTE Model'\n",
    "\n",
    "# Oversample subsampled_X, subsampled_y using SMOTE\n",
    "sm = SMOTE(random_state=4, k_neighbors=5)\n",
    "oversampled_X, oversampled_y = sm.fit_resample(train_X, train_y)\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "oversampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                  random_state=4, n_estimators=1000)\n",
    "# fit model using oversampled train data\n",
    "oversampled_model.fit(oversampled_X, oversampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "oversampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': oversampled_y.value_counts()[0],\n",
    "  'resampled_N1': oversampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "update"
    ]
   },
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = oversampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = oversampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")\n",
    "# forecasts_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj. Oversampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Adj. Oversampled SMOTE Model'\n",
    "\n",
    "# Oversample subsampled_X, subsampled_y using SMOTE\n",
    "sm = SMOTE(random_state=4, k_neighbors=5)\n",
    "oversampled_X, oversampled_y = sm.fit_resample(train_X, train_y)\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "oversampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                  random_state=4, n_estimators=1000)\n",
    "# fit model using oversampled train data\n",
    "oversampled_model.fit(oversampled_X, oversampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "oversampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': oversampled_y.value_counts()[0],\n",
    "  'resampled_N1': oversampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = oversampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = oversampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "# Adjust forecasts probability\n",
    "forecasts_proba = adjust_class(forecasts_proba,\n",
    "                               oversampled_y.values.mean(),\n",
    "                               train_y.mean())\n",
    "\n",
    "# Convert probabilities to classes\n",
    "forecasts = (forecasts_proba > 0.5).astype(int)\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampled+Oversampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Undersampled+Oversampled Model'\n",
    "\n",
    "######################\n",
    "nm = NearMiss()\n",
    "undersampled_X, undersampled_y = nm.fit_resample(train_X, train_y)\n",
    "# Combine so that we can filter together\n",
    "undersampled_train = pd.concat([undersampled_X, undersampled_y], axis=1)\n",
    "# Get rows with wearing_off(t+4) == 0\n",
    "undersampled_train = undersampled_train[undersampled_train.iloc[:, -1] == 0].copy()\n",
    "\n",
    "######################\n",
    "sm = SMOTE(random_state=4, k_neighbors=5)\n",
    "oversampled_X, oversampled_y = sm.fit_resample(train_X, train_y)\n",
    "# Combine so that we can filter together\n",
    "oversampled_train = pd.concat([oversampled_X, oversampled_y], axis=1)\n",
    "# Get rows with wearing_off(t+4) == 1\n",
    "oversampled_train = oversampled_train[oversampled_train.iloc[:, -1] == 1].copy()\n",
    "\n",
    "######################\n",
    "resampled_train = pd.concat([undersampled_train, oversampled_train], axis=0)\n",
    "resampled_X, resampled_y = split_x_y(\n",
    "    resampled_train, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "resampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                random_state=4, n_estimators=1000)\n",
    "# fit model using resampled train data\n",
    "resampled_model.fit(resampled_X, resampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "resampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': resampled_y.value_counts()[0],\n",
    "  'resampled_N1': resampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "update"
    ]
   },
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = resampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = resampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")\n",
    "# forecasts_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj. Undersampled+Oversampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Adj. Undersampled+Oversampled Model'\n",
    "\n",
    "nm = NearMiss()\n",
    "\n",
    "undersampled_X, undersampled_y = nm.fit_resample(train_X, train_y)\n",
    "# Combine so that we can filter together\n",
    "undersampled_train = pd.concat([undersampled_X, undersampled_y], axis=1)\n",
    "# Get rows with wearing_off(t+4) == 0\n",
    "undersampled_train = undersampled_train[undersampled_train.iloc[:, -1] == 0].copy()\n",
    "\n",
    "sm = SMOTE(random_state=4, k_neighbors=5)\n",
    "\n",
    "oversampled_X, oversampled_y = sm.fit_resample(train_X, train_y)\n",
    "# Combine so that we can filter together\n",
    "oversampled_train = pd.concat([oversampled_X, oversampled_y], axis=1)\n",
    "# Get rows with wearing_off(t+4) == 1\n",
    "oversampled_train = oversampled_train[oversampled_train.iloc[:, -1] == 1].copy()\n",
    "\n",
    "resampled_train = pd.concat([undersampled_train, oversampled_train], axis=0)\n",
    "resampled_X, resampled_y = split_x_y(\n",
    "    resampled_train, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "resampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                random_state=4, n_estimators=1000)\n",
    "# fit model using resampled train data\n",
    "resampled_model.fit(resampled_X, resampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "resampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': resampled_y.value_counts()[0],\n",
    "  'resampled_N1': resampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = oversampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = oversampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "# Adjust forecasts probability\n",
    "forecasts_proba = adjust_class(forecasts_proba,\n",
    "                               resampled_y.values.mean(),\n",
    "                               train_y.mean())\n",
    "\n",
    "# Convert probabilities to classes\n",
    "forecasts = (forecasts_proba > 0.5).astype(int)\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTETomek Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SMOTETomek Resampled Model'\n",
    "\n",
    "smote_tomek = SMOTETomek(random_state=4)\n",
    "\n",
    "resampled_X, resampled_y = sm.fit_resample(train_X, train_y)\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "resampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                random_state=4, n_estimators=1000)\n",
    "# fit model using resampled train data\n",
    "resampled_model.fit(resampled_X, resampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "resampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': resampled_y.value_counts()[0],\n",
    "  'resampled_N1': resampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = resampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = resampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")\n",
    "# forecasts_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj. SMOTETomek Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Adj. SMOTETomek Resampled Model'\n",
    "\n",
    "smote_tomek = SMOTETomek(random_state=4)\n",
    "\n",
    "resampled_X, resampled_y = sm.fit_resample(train_X, train_y)\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "resampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                random_state=4, n_estimators=1000)\n",
    "# fit model using resampled train data\n",
    "resampled_model.fit(resampled_X, resampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "resampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': resampled_y.value_counts()[0],\n",
    "  'resampled_N1': resampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = oversampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = oversampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "# Adjust forecasts probability\n",
    "forecasts_proba = adjust_class(forecasts_proba,\n",
    "                               resampled_y.values.mean(),\n",
    "                               train_y.mean())\n",
    "\n",
    "# Convert probabilities to classes\n",
    "forecasts = (forecasts_proba > 0.5).astype(int)\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
