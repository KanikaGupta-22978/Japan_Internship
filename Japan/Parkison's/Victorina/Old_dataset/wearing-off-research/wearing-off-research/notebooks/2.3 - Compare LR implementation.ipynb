{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Objective Function for XGBoost\n",
    "\n",
    "Noel's notes:\n",
    "* Working as expected with the custom objective function\n",
    "* Has the same output as XGBClassifier on either `predict()` or `predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&lt;function logistic_obj at 0x7f28a86e20e0&gt;,\n",
       "              predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&lt;function logistic_obj at 0x7f28a86e20e0&gt;,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=<function logistic_obj at 0x7f28a86e20e0>,\n",
       "              predictor=None, ...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "\n",
    "def logistic_obj(labels: np.ndarray, predt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "  '''\n",
    "  Logistic loss objective function for binary-class classification\n",
    "  '''\n",
    "  y = labels\n",
    "  p = sigmoid(predt)\n",
    "  grad = p - y\n",
    "  hess = p * (1.0 - p)\n",
    "\n",
    "  return grad, hess\n",
    "\n",
    "\n",
    "# Create XGBClassifier model instance with custom objective function\n",
    "clf = XGBClassifier(random_state=4, n_estimators=1000, objective=logistic_obj)\n",
    "\n",
    "# fit model\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBClassifier model instance\n",
    "base_model = XGBClassifier(random_state=4, n_estimators=1000)\n",
    "\n",
    "# fit model\n",
    "base_model.fit(train_X, train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import DMatrix\n",
    "\n",
    "sample = test_X.sample()\n",
    "\n",
    "print(f'''\n",
    "Customized XGBClassifier\n",
    "{clf.predict(sample)}\n",
    "{clf.predict_proba(sample)}\n",
    "\n",
    "Base XGBClassifier\n",
    "{base_model.predict(sample)}\n",
    "{base_model.predict_proba(sample)}\n",
    "{base_model.get_booster().predict(DMatrix(sample))} # This is actually the source for predict_proba() method\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through test_X\n",
    "hit_for_predict_proba = False\n",
    "hit_for_predict = False\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "  # get sample\n",
    "  sample = test_X.iloc[i:i + 1]\n",
    "\n",
    "  # compare predictions\n",
    "  if clf.predict_proba(sample)[:, 1] != base_model.predict_proba(sample)[:, 1]:\n",
    "    print(clf.predict_proba(sample))\n",
    "    print(base_model.predict_proba(sample))\n",
    "    print()\n",
    "    hit_for_predict_proba = True\n",
    "  if clf.predict(sample) != base_model.predict(sample):\n",
    "    print(clf.predict(sample))\n",
    "    print(base_model.predict(sample))\n",
    "    hit_for_predict = True\n",
    "\n",
    "if not hit_for_predict:\n",
    "  print('Success! Same output in terms of predict()')\n",
    "if not hit_for_predict_proba:\n",
    "  print('Success! Same output in terms of predict_proba()')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Objective Function\n",
    "\n",
    "Noel's notes:\n",
    "* Working as expected with the custom objective function\n",
    "* But, I want to customize `y_pred` in `custom_objective()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom objective function\n",
    "def custom_objective(y_true, y_pred):\n",
    "  # Customize the objective function calculation here\n",
    "  # y_true: array-like, shape (n_samples,)\n",
    "  # y_pred: array-like, shape (n_samples, n_classes)\n",
    "  # Return a scalar representing the objective function value\n",
    "\n",
    "  # Example: Negative log-likelihood\n",
    "  return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "# Create a LogisticRegression object\n",
    "custom_obj_model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Set the scoring function to your custom objective function\n",
    "custom_obj_model.score = custom_objective\n",
    "\n",
    "# Fit the model to your data\n",
    "custom_obj_model.fit(train_X_scaled_normalized, train_y.values.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = custom_obj_model.predict(\n",
    "  test_X_scaled_normalized\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = custom_obj_model.predict_proba(\n",
    "  test_X_scaled_normalized\n",
    ")[:, 1]\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth', 'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X_scaled_normalized.index\n",
    ")\n",
    "# forecasts_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "From this part, we're showing how the forecasts_output will be evaluated for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot `test_y.values.ravel()` and `preds_proba` on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# y-axis label Wearing-off Forecast Probability\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# title\n",
    "plt.title(f'Custom Objective Function Model for {USER.upper()}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "display(\n",
    "  pd.DataFrame(\n",
    "    [\n",
    "      metrics.f1_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off),\n",
    "      metrics.recall_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off),\n",
    "      metrics.precision_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off),\n",
    "      metrics.accuracy_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off)\n",
    "    ],\n",
    "    index=['f1 score', 'recall', 'precision', 'accuracy'],\n",
    "    columns=['metrics']\n",
    "  ).T\n",
    ")\n",
    "pd.DataFrame(classification_report(\n",
    "  forecasts_output.ground_truth,\n",
    "  forecasts_output.forecasted_wearing_off,\n",
    "  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "LABELS = ['No Wearing-off', 'Wearing-off']\n",
    "\n",
    "conf_matrix = confusion_matrix(test_y.values.ravel(),\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=(FIGSIZE_CM))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS,\n",
    "            yticklabels=LABELS, annot=True, fmt=\".2f\")\n",
    "plt.title(\n",
    "    f\"Custom Objective Function Model's confusion matrix for {USER.upper()}\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model\n",
    "\n",
    "Noel's notes:\n",
    "* Working but does not have the same output as original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def negative_log_likelihood(theta, X, y, alpha):\n",
    "  m = X.shape[0]  # number of training examples\n",
    "  z = np.dot(X, theta)\n",
    "  h = sigmoid(z)\n",
    "  cost = -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) / m\n",
    "  # Add L2 penalty to the cost function\n",
    "  #   Exclude theta[0] from regularization\n",
    "  regularization_term = (alpha / (2 * m)) * np.sum(theta[1:]**2)\n",
    "  cost += regularization_term\n",
    "  return cost\n",
    "\n",
    "\n",
    "def gradient(theta, X, y, alpha):\n",
    "  m = X.shape[0]  # number of training examples\n",
    "  z = np.dot(X, theta)\n",
    "  h = sigmoid(z)\n",
    "  grad = np.dot(X.T, (h - y)) / m\n",
    "  # L2 penalty term\n",
    "  regularization_term = (alpha / m) * theta[1:]\n",
    "  grad[1:] += regularization_term\n",
    "  return grad\n",
    "\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "  def __init__(self, alpha=0.0):\n",
    "    self.theta = None\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    m, n = X.shape\n",
    "    self.theta = np.zeros(n)\n",
    "\n",
    "    result = minimize(\n",
    "        fun=negative_log_likelihood,\n",
    "        x0=self.theta,\n",
    "        args=(X, y, self.alpha),\n",
    "        method='L-BFGS-B',\n",
    "        jac=gradient,\n",
    "        options={'maxiter': 200}\n",
    "    )\n",
    "\n",
    "    self.theta = result.x\n",
    "\n",
    "  def predict(self, X):\n",
    "    z = np.dot(X, self.theta)\n",
    "    predictions = sigmoid(z)\n",
    "    return np.round(predictions)\n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    z = np.dot(X, self.theta)\n",
    "    predictions = sigmoid(z)\n",
    "\n",
    "    # Return the probability for the 0s and 1s class\n",
    "    predictions_proba = np.zeros((len(predictions), 2))\n",
    "    predictions_proba[:, 0] = 1 - predictions\n",
    "    predictions_proba[:, 1] = predictions\n",
    "\n",
    "    return predictions_proba\n",
    "\n",
    "\n",
    "custom_lr_model = CustomLogisticRegression(alpha=0.1)\n",
    "custom_lr_model.fit(train_X_scaled_normalized, train_y.values.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = custom_lr_model.predict(\n",
    "  test_X_scaled_normalized\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = custom_lr_model.predict_proba(\n",
    "  test_X_scaled_normalized\n",
    ")[:, 1]\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth', 'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X_scaled_normalized.index\n",
    ")\n",
    "# forecasts_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "From this part, we're showing how the forecasts_output will be evaluated for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot `test_y.values.ravel()` and `preds_proba` on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# y-axis label Wearing-off Forecast Probability\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# title\n",
    "plt.title(f'Custom LR Model for {USER.upper()}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "display(\n",
    "  pd.DataFrame(\n",
    "    [\n",
    "      metrics.f1_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off),\n",
    "      metrics.recall_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off),\n",
    "      metrics.precision_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off),\n",
    "      metrics.accuracy_score(\n",
    "        forecasts_output.ground_truth,\n",
    "        forecasts_output.forecasted_wearing_off)\n",
    "    ],\n",
    "    index=['f1 score', 'recall', 'precision', 'accuracy'],\n",
    "    columns=['metrics']\n",
    "  ).T\n",
    ")\n",
    "pd.DataFrame(classification_report(\n",
    "  forecasts_output.ground_truth,\n",
    "  forecasts_output.forecasted_wearing_off,\n",
    "  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "LABELS = ['No Wearing-off', 'Wearing-off']\n",
    "\n",
    "conf_matrix = confusion_matrix(test_y.values.ravel(),\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=(FIGSIZE_CM))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS,\n",
    "            yticklabels=LABELS, annot=True, fmt=\".2f\")\n",
    "plt.title(\n",
    "    f\"Custom LR Model's confusion matrix for {USER.upper()}\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
