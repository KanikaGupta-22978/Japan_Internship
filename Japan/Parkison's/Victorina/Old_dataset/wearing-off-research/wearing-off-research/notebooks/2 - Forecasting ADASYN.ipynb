{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Wearing-off\n",
    "\n",
    "References:\n",
    "* [Machine Learning Mastery's Time Series Tutorial](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/)\n",
    "* [Tensorflow's Time Series Tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from openTSNE import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# For evaluation\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# For resampling\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant to process\n",
    "USER = 'participant1'\n",
    "# USER = f'participant{sys.argv[1]}'\n",
    "\n",
    "# Collection dataset\n",
    "# COLLECTION = '2-person'\n",
    "COLLECTION = '10-person'\n",
    "# COLLECTION = '3-person'\n",
    "\n",
    "# Define base path\n",
    "BASE_DATA_PATH = '/workspaces/data'\n",
    "\n",
    "# Define results path\n",
    "RESULTS_PATH = '/workspaces/results'\n",
    "\n",
    "FIGSIZE = (20, 7)\n",
    "FIGSIZE_CM = (13, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features\n",
    "# Garmin features\n",
    "features = ['heart_rate', 'steps', 'stress_score',\n",
    "            'awake', 'deep', 'light', 'rem',\n",
    "            'nonrem_total', 'total', 'nonrem_percentage', 'sleep_efficiency']\n",
    "\n",
    "# FonLog features\n",
    "# features += ['time_from_last_drug_taken', 'wo_duration']\n",
    "\n",
    "# Additional features\n",
    "# features += ['timestamp_hour', 'timestamp_dayofweek',\n",
    "features += ['timestamp_dayofweek',\n",
    "             'timestamp_hour_sin', 'timestamp_hour_cos']\n",
    "\n",
    "TARGET_COLUMN = 'wearing_off'\n",
    "features.append(TARGET_COLUMN)\n",
    "\n",
    "columns = ['timestamp'] + features + ['participant']\n",
    "\n",
    "# Normalize features\n",
    "normalize_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics & Other Hyperparameters\n",
    "SHIFT = 4\n",
    "RECORD_SIZE_PER_DAY = 96  # 60 minutes / 15 minutes * 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set periods\n",
    "test_set_horizons = {\n",
    "  \"participant1\": [\"2021-12-02 0:00\", \"2021-12-03 23:45\"],\n",
    "  \"participant2\": [\"2021-11-28 0:00\", \"2021-11-29 23:45\"],\n",
    "  \"participant3\": [\"2021-11-25 0:00\", \"2021-11-26 23:45\"],\n",
    "  \"participant4\": [\"2021-12-06 0:00\", \"2021-12-07 7:15\"],\n",
    "  \"participant5\": [\"2021-11-28 0:00\", \"2021-11-29 23:45\"],\n",
    "  \"participant6\": [\"2021-12-06 0:00\", \"2021-12-07 23:45\"],\n",
    "  \"participant7\": [\"2021-12-12 0:00\", \"2021-12-13 9:45\"],\n",
    "  \"participant8\": [\"2021-12-23 0:00\", \"2021-12-24 23:45\"],\n",
    "  \"participant9\": [\"2021-12-23 0:00\", \"2021-12-24 23:45\"],\n",
    "  \"participant10\": [\"2021-12-23 0:00\", \"2021-12-24 23:45\"],\n",
    "  \"participant11\": [\"2023-01-30 0:00\", \"2023-01-31 23:45\"],\n",
    "  \"participant12\": [\"2023-01-10 0:00\", \"2023-01-11 23:45\"],\n",
    "  \"participant13\": [\"2023-01-29 0:00\", \"2023-01-30 23:45\"],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant's Excel file\n",
    "dataset = pd.read_excel(f'{BASE_DATA_PATH}/{COLLECTION}/combined_data.xlsx',\n",
    "                        index_col=\"timestamp\",\n",
    "                        usecols=columns,\n",
    "                        engine='openpyxl')\n",
    "\n",
    "# Fill missing data with 0\n",
    "dataset.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant's Excel file\n",
    "dataset2 = pd.read_excel(f'{BASE_DATA_PATH}/3-person/combined_data.xlsx',\n",
    "                         index_col=\"timestamp\",\n",
    "                         usecols=columns,\n",
    "                         engine='openpyxl')\n",
    "\n",
    "# Fill missing data with 0\n",
    "dataset2.fillna(0, inplace=True)\n",
    "\n",
    "# Combine datasets\n",
    "dataset = pd.concat([dataset, dataset2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a copy of the original dataset\n",
    "original_dataset = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'participant13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by participant\n",
    "dataset = original_dataset.query(f'participant == \"{USER}\"').drop(\n",
    "    columns=['participant']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset.loc[\n",
    "  dataset.index < test_set_horizons[USER][0]\n",
    "].copy()\n",
    "test_df = dataset.loc[test_set_horizons[USER][0]:].copy()\n",
    "\n",
    "# # Divide train_df to train_df and validation_df where validation_df is the last 20% of train_df\n",
    "# validation_df = train_df.iloc[int(len(train_df) * 0.8):].copy()\n",
    "# train_df = train_df.iloc[:int(len(train_df) * 0.8)].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform series to supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "  var_names = data.columns\n",
    "  n_vars = len(var_names)\n",
    "  df = pd.DataFrame(data)\n",
    "  cols, names = list(), list()  # new column values, new columne names\n",
    "\n",
    "  # input sequence (t-i, ... t-1)\n",
    "  # timesteps before (e.g., n_in = 3, t-3, t-2, t-1)\n",
    "  for i in range(n_in, 0, -1):\n",
    "    cols.append(df.shift(i))\n",
    "    names += list(\n",
    "        map(lambda var_name: f'{var_name}(t-{i})', var_names)\n",
    "    )\n",
    "\n",
    "  # forecast sequence (t, t+1, ... t+n)\n",
    "  # timesteps after (e.g., n_out = 3, t, t+1, t+2)\n",
    "  for i in range(0, n_out):\n",
    "    cols.append(df.shift(-i))\n",
    "    if i == 0:\n",
    "      names += list(map(lambda var_name: f'{var_name}(t)', var_names))\n",
    "    else:\n",
    "      names += list(map(lambda var_name: f'{var_name}(t+{i})', var_names))\n",
    "\n",
    "  # put it all together\n",
    "  agg = pd.concat(cols, axis=1)\n",
    "  agg.columns = names\n",
    "\n",
    "  # drop rows with NaN values\n",
    "  if dropnan:\n",
    "    agg.dropna(inplace=True)\n",
    "\n",
    "  return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "def split_x_y(df, target_columns, SHIFT=SHIFT):\n",
    "  # Drop extra columns i.e., (t+1), (t+2), (t+3), (t+4)\n",
    "  regex = r\".*\\(t\\+[1-{SHIFT}]\\)$\"  # includes data(t)\n",
    "  # regex = r\"\\(t(\\+([1-{SHIFT}]))?\\)$\" # removes data(t)\n",
    "\n",
    "  # Drop extra columns except target_columns\n",
    "  df.drop(\n",
    "    [x for x in df.columns if re.search(regex, x) and x not in target_columns],\n",
    "    axis=1, inplace=True\n",
    "  )\n",
    "\n",
    "  # Split into X and y\n",
    "  y = df[target_columns].copy()\n",
    "  X = df.drop(target_columns, axis=1)\n",
    "\n",
    "  return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IN = SHIFT  # Last hour\n",
    "N_OUT = SHIFT + 1  # Next hour\n",
    "\n",
    "# For a similar sliding window with TF's WindowGenerator,\n",
    "#   n_in = last day, t - 47\n",
    "#   n_out = next 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train set to supervised learning\n",
    "reframed_train_df = series_to_supervised(train_df,\n",
    "                                         n_in=N_IN,\n",
    "                                         n_out=N_OUT,\n",
    "                                         dropnan=True)\n",
    "\n",
    "train_X, train_y = split_x_y(\n",
    "    reframed_train_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "\n",
    "# display(train_y.head(5))\n",
    "# display(train_X.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with validation data's N_IN tail\\nreframed_test_df = series_to_supervised(pd.concat([validation_df.tail(N_IN),\\n                                                   test_df,\\n                                                   ]),\\n                                        n_in=N_IN,\\n                                        n_out=N_OUT,\\n                                        dropnan=True)\\ntest_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert test set to supervised learning\n",
    "# with train data's N_IN tail\n",
    "reframed_test_df = series_to_supervised(pd.concat([train_df.tail(N_IN),\n",
    "                                                   test_df,\n",
    "                                                   ]),\n",
    "                                        n_in=N_IN,\n",
    "                                        n_out=N_OUT,\n",
    "                                        dropnan=True)\n",
    "test_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "\n",
    "'''test data only\n",
    "reframed_test_df = series_to_supervised(test_df,\n",
    "                                        n_in=N_IN,\n",
    "                                        n_out=N_OUT,\n",
    "                                        dropnan=True)\n",
    "test_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{SHIFT})'])\n",
    "'''\n",
    "\n",
    "'''with validation data's N_IN tail\n",
    "reframed_test_df = series_to_supervised(pd.concat([validation_df.tail(N_IN),\n",
    "                                                   test_df,\n",
    "                                                   ]),\n",
    "                                        n_in=N_IN,\n",
    "                                        n_out=N_OUT,\n",
    "                                        dropnan=True)\n",
    "test_X, test_y = split_x_y(reframed_test_df, [f'{TARGET_COLUMN}(t+{N_OUT-1})'])\n",
    "'''\n",
    "\n",
    "# display(test_y.head(5))\n",
    "# display(test_X.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "train_X_scaled = pd.DataFrame(train_X_scaled,\n",
    "                              columns=train_X.columns,\n",
    "                              index=train_X.index)\n",
    "test_X_scaled = scaler.fit_transform(test_X)\n",
    "test_X_scaled = pd.DataFrame(test_X_scaled,\n",
    "                             columns=test_X.columns,\n",
    "                             index=test_X.index)\n",
    "\n",
    "# display(test_X_scaled.head(5))\n",
    "# display(train_X_scaled.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "normalizer = Normalizer()\n",
    "train_X_scaled_normalized = normalizer.fit_transform(train_X_scaled)\n",
    "\n",
    "train_X_scaled_normalized = pd.DataFrame(train_X_scaled_normalized,\n",
    "                                         columns=train_X.columns,\n",
    "                                         index=train_X.index)\n",
    "\n",
    "test_X_scaled_normalized = normalizer.fit_transform(test_X_scaled)\n",
    "\n",
    "test_X_scaled_normalized = pd.DataFrame(test_X_scaled_normalized,\n",
    "                                        columns=test_X.columns,\n",
    "                                        index=test_X.index)\n",
    "\n",
    "# display(train_X_scaled_normalized.head(5))\n",
    "# display(test_X_scaled_normalized.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep original data\n",
    "original_train_X_scaled_normalized = train_X_scaled_normalized.copy()\n",
    "original_train_y = train_y.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heart_rate(t-4)</th>\n",
       "      <th>steps(t-4)</th>\n",
       "      <th>stress_score(t-4)</th>\n",
       "      <th>awake(t-4)</th>\n",
       "      <th>deep(t-4)</th>\n",
       "      <th>light(t-4)</th>\n",
       "      <th>rem(t-4)</th>\n",
       "      <th>nonrem_total(t-4)</th>\n",
       "      <th>total(t-4)</th>\n",
       "      <th>nonrem_percentage(t-4)</th>\n",
       "      <th>...</th>\n",
       "      <th>light(t)</th>\n",
       "      <th>rem(t)</th>\n",
       "      <th>nonrem_total(t)</th>\n",
       "      <th>total(t)</th>\n",
       "      <th>nonrem_percentage(t)</th>\n",
       "      <th>sleep_efficiency(t)</th>\n",
       "      <th>timestamp_dayofweek(t)</th>\n",
       "      <th>timestamp_hour_sin(t)</th>\n",
       "      <th>timestamp_hour_cos(t)</th>\n",
       "      <th>wearing_off(t)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-26 14:00:00</th>\n",
       "      <td>0.166881</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.121273</td>\n",
       "      <td>0.025559</td>\n",
       "      <td>0.106392</td>\n",
       "      <td>0.128389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139846</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>0.204471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139846</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>0.204471</td>\n",
       "      <td>0.174685</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.051118</td>\n",
       "      <td>0.013697</td>\n",
       "      <td>0.204471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02 01:15:00</th>\n",
       "      <td>0.130889</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.056182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.052505</td>\n",
       "      <td>0.046949</td>\n",
       "      <td>0.145107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.052505</td>\n",
       "      <td>0.046949</td>\n",
       "      <td>0.145107</td>\n",
       "      <td>0.198694</td>\n",
       "      <td>0.132463</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.193422</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-04 11:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.032094</td>\n",
       "      <td>0.066424</td>\n",
       "      <td>0.169829</td>\n",
       "      <td>0.120055</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.186410</td>\n",
       "      <td>0.086689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169829</td>\n",
       "      <td>0.120055</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.186410</td>\n",
       "      <td>0.086689</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>0.201731</td>\n",
       "      <td>0.126972</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-17 23:15:00</th>\n",
       "      <td>0.161463</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125233</td>\n",
       "      <td>0.142749</td>\n",
       "      <td>0.118154</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>0.186788</td>\n",
       "      <td>0.158415</td>\n",
       "      <td>0.148756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118154</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>0.186788</td>\n",
       "      <td>0.158415</td>\n",
       "      <td>0.148756</td>\n",
       "      <td>0.081353</td>\n",
       "      <td>0.031131</td>\n",
       "      <td>0.075174</td>\n",
       "      <td>0.184993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-25 05:15:00</th>\n",
       "      <td>0.106058</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.038754</td>\n",
       "      <td>0.057676</td>\n",
       "      <td>0.128214</td>\n",
       "      <td>0.100330</td>\n",
       "      <td>0.023874</td>\n",
       "      <td>0.152152</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.155055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100330</td>\n",
       "      <td>0.023874</td>\n",
       "      <td>0.152152</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.155055</td>\n",
       "      <td>0.124703</td>\n",
       "      <td>0.181268</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.108316</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     heart_rate(t-4)  steps(t-4)  stress_score(t-4)  \\\n",
       "timestamp                                                             \n",
       "2023-01-26 14:00:00         0.166881    0.006711           0.121273   \n",
       "2022-12-02 01:15:00         0.130889    0.000133           0.056182   \n",
       "2022-12-04 11:00:00         0.000000    0.000135           0.002319   \n",
       "2023-01-17 23:15:00         0.161463    0.021394           0.000000   \n",
       "2022-12-25 05:15:00         0.106058    0.000121           0.038754   \n",
       "\n",
       "                     awake(t-4)  deep(t-4)  light(t-4)  rem(t-4)  \\\n",
       "timestamp                                                          \n",
       "2023-01-26 14:00:00    0.025559   0.106392    0.128389  0.000000   \n",
       "2022-12-02 01:15:00    0.000000   0.147001    0.000000  0.037800   \n",
       "2022-12-04 11:00:00    0.032094   0.066424    0.169829  0.120055   \n",
       "2023-01-17 23:15:00    0.125233   0.142749    0.118154  0.038269   \n",
       "2022-12-25 05:15:00    0.057676   0.128214    0.100330  0.023874   \n",
       "\n",
       "                     nonrem_total(t-4)  total(t-4)  nonrem_percentage(t-4)  \\\n",
       "timestamp                                                                    \n",
       "2023-01-26 14:00:00           0.139846    0.084549                0.204471   \n",
       "2022-12-02 01:15:00           0.052505    0.046949                0.145107   \n",
       "2022-12-04 11:00:00           0.136926    0.186410                0.086689   \n",
       "2023-01-17 23:15:00           0.186788    0.158415                0.148756   \n",
       "2022-12-25 05:15:00           0.152152    0.118551                0.155055   \n",
       "\n",
       "                     ...  light(t)    rem(t)  nonrem_total(t)  total(t)  \\\n",
       "timestamp            ...                                                  \n",
       "2023-01-26 14:00:00  ...  0.128389  0.000000         0.139846  0.084549   \n",
       "2022-12-02 01:15:00  ...  0.000000  0.037800         0.052505  0.046949   \n",
       "2022-12-04 11:00:00  ...  0.169829  0.120055         0.136926  0.186410   \n",
       "2023-01-17 23:15:00  ...  0.118154  0.038269         0.186788  0.158415   \n",
       "2022-12-25 05:15:00  ...  0.100330  0.023874         0.152152  0.118551   \n",
       "\n",
       "                     nonrem_percentage(t)  sleep_efficiency(t)  \\\n",
       "timestamp                                                        \n",
       "2023-01-26 14:00:00              0.204471             0.174685   \n",
       "2022-12-02 01:15:00              0.145107             0.198694   \n",
       "2022-12-04 11:00:00              0.086689             0.173300   \n",
       "2023-01-17 23:15:00              0.148756             0.081353   \n",
       "2022-12-25 05:15:00              0.155055             0.124703   \n",
       "\n",
       "                     timestamp_dayofweek(t)  timestamp_hour_sin(t)  \\\n",
       "timestamp                                                            \n",
       "2023-01-26 14:00:00                0.102236               0.051118   \n",
       "2022-12-02 01:15:00                0.132463               0.131281   \n",
       "2022-12-04 11:00:00                0.201731               0.126972   \n",
       "2023-01-17 23:15:00                0.031131               0.075174   \n",
       "2022-12-25 05:15:00                0.181268               0.179526   \n",
       "\n",
       "                     timestamp_hour_cos(t)  wearing_off(t)  \n",
       "timestamp                                                   \n",
       "2023-01-26 14:00:00               0.013697        0.204471  \n",
       "2022-12-02 01:15:00               0.193422        0.000000  \n",
       "2022-12-04 11:00:00               0.003437        0.000000  \n",
       "2023-01-17 23:15:00               0.184993        0.000000  \n",
       "2022-12-25 05:15:00               0.108316        0.000000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle train dataset\n",
    "# Combine train_y to train_X_scaled_normalized\n",
    "train = pd.concat(\n",
    "    [original_train_X_scaled_normalized, original_train_y], axis=1)\n",
    "\n",
    "# Shuffle train\n",
    "train = train.sample(frac=1, random_state=4)\n",
    "\n",
    "# Split train into X and y\n",
    "train_y = train[f'{TARGET_COLUMN}(t+{SHIFT})']\n",
    "train_X_scaled_normalized = train.drop(f'{TARGET_COLUMN}(t+{SHIFT})', axis=1)\n",
    "\n",
    "# Original train_X, train_y\n",
    "original_train_X = train_X.copy()\n",
    "original_test_X = test_X.copy()\n",
    "\n",
    "# Renamed to train_X, train_y for easier reference\n",
    "train_X = train_X_scaled_normalized.copy()\n",
    "test_X = test_X_scaled_normalized.copy()\n",
    "\n",
    "train_X.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from scipy.special import expit as sigmoid\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def logistic_obj(labels: np.ndarray, predt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "  '''\n",
    "  Logistic loss objective function for binary-class classification\n",
    "  '''\n",
    "  # grad = grad.flatten()\n",
    "  # hess = hess.flatten()\n",
    "  # return grad, hess\n",
    "  y = labels\n",
    "  p = sigmoid(predt)\n",
    "  grad = p - y\n",
    "  hess = p * (1.0 - p)\n",
    "\n",
    "  return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_class(conditional_probability, wrongprob, trueprob):\n",
    "  a = conditional_probability / (wrongprob / trueprob)\n",
    "  comp_cond = 1 - conditional_probability\n",
    "  comp_wrong = 1 - wrongprob\n",
    "  comp_true = 1 - trueprob\n",
    "  b = comp_cond / (comp_wrong / comp_true)\n",
    "  return a / (a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "if not os.path.isfile(f'{RESULTS_PATH}/metric scores.xlsx'):\n",
    "  # Create ExcelWriter\n",
    "  writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                          engine='openpyxl', mode='w')\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Metric Scores\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model',\n",
    "             'f1 score', 'recall', 'precision', 'accuracy',\n",
    "             'auc-roc', 'auc-prc']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Metric Scores', index=False)\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Classification Report\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model', 'classification report',\n",
    "             'precision', 'recall', 'f1-score', 'support']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Classification Report', index=False)\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Confusion Matrix\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Confusion Matrix', index=False)\n",
    "\n",
    "  # Create an empty DataFrame to write to Excel for Sampling Ratio\n",
    "  pd.DataFrame(\n",
    "    columns=['participant', 'model', 'original_N0', 'original_N1', 'resampled_N0',\n",
    "             'resampled_N1', 'sampling_rate']\n",
    "  ).to_excel(excel_writer=writer, sheet_name='Sampling Ratio', index=False)\n",
    "\n",
    "  writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADASYN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ADASYN Model'\n",
    "\n",
    "# Oversample subsampled_X, subsampled_y using ADASYN\n",
    "ada = ADASYN(random_state=4, n_neighbors=5)\n",
    "oversampled_X, oversampled_y = ada.fit_resample(train_X, train_y)\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "oversampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                  random_state=4, n_estimators=1000)\n",
    "# fit model using oversampled train data\n",
    "oversampled_model.fit(oversampled_X, oversampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "oversampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': oversampled_y.value_counts()[0],\n",
    "  'resampled_N1': oversampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "update"
    ]
   },
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = oversampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = oversampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")\n",
    "# forecasts_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj. ADASYN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Adj. ADASYN Model'\n",
    "\n",
    "# Oversample subsampled_X, subsampled_y using ADASYN\n",
    "ada = ADASYN(random_state=4, n_neighbors=5)\n",
    "oversampled_X, oversampled_y = ada.fit_resample(train_X, train_y)\n",
    "\n",
    "# Create XGBClassifier with custom objective function model instance\n",
    "oversampled_model = XGBClassifier(objective=logistic_obj,\n",
    "                                  random_state=4, n_estimators=1000)\n",
    "# fit model using oversampled train data\n",
    "oversampled_model.fit(oversampled_X, oversampled_y)\n",
    "\n",
    "# save XGBClassifier model\n",
    "oversampled_model.save_model(f'{RESULTS_PATH}/{USER}, {model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these two into one dataframe\n",
    "ratios = pd.DataFrame({\n",
    "  'original_N0': train_y.value_counts()[0],\n",
    "  'original_N1': train_y.value_counts()[1],\n",
    "  'resampled_N0': oversampled_y.value_counts()[0],\n",
    "  'resampled_N1': oversampled_y.value_counts()[1]\n",
    "}, index=[0]).assign(participant=USER, model=model_name)\n",
    "\n",
    "ratios.set_index(['participant', 'model'], inplace=True)\n",
    "ratios.reset_index(inplace=True)\n",
    "\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "ratios.to_excel(excel_writer=writer, sheet_name='Sampling Ratio',\n",
    "                startrow=writer.sheets['Sampling Ratio'].max_row,\n",
    "                header=False, index=False)\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts\n",
    "forecasts = oversampled_model.predict(\n",
    "  test_X\n",
    ")\n",
    "\n",
    "# Get the probability for 1s class\n",
    "forecasts_proba = oversampled_model.predict_proba(\n",
    "  test_X\n",
    ")[:, 1]\n",
    "\n",
    "# Adjust forecasts probability\n",
    "forecasts_proba = adjust_class(forecasts_proba,\n",
    "                               oversampled_y.values.mean(),\n",
    "                               train_y.mean())\n",
    "\n",
    "# Convert probabilities to classes\n",
    "forecasts = (forecasts_proba > 0.5).astype(int)\n",
    "\n",
    "forecasts_output = pd.DataFrame(\n",
    "  {\n",
    "    'patient_id': [USER] * len(forecasts),\n",
    "    'ground_truth': test_y.values.ravel(),\n",
    "    'forecasted_wearing_off': forecasts,\n",
    "    'forecasted_wearing_off_probability': forecasts_proba\n",
    "  },\n",
    "  columns=['patient_id', 'ground_truth',\n",
    "           'forecasted_wearing_off',\n",
    "           'forecasted_wearing_off_probability'],\n",
    "  index=test_X.index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth, and predicted probability on the same plot to show the difference\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(forecasts_output.ground_truth,\n",
    "         label='actual', color='red', marker='o',)\n",
    "plt.plot(forecasts_output.forecasted_wearing_off_probability,\n",
    "         label='predicted', color='blue', marker='o')\n",
    "# plt.plot(forecasts_output.forecasted_wearing_off,\n",
    "#          label='predicted', color='blue', marker='o')\n",
    "plt.legend()\n",
    "\n",
    "# Dashed horizontal line at 0.5\n",
    "plt.axhline(0.5, linestyle='--', color='gray')\n",
    "\n",
    "# Dashed vertical lines on each hour\n",
    "for i in forecasts_output.index:\n",
    "  if pd.Timestamp(i).minute == 0:\n",
    "    plt.axvline(i, linestyle='--', color='gray')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Wearing-off Forecast Probability')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s Forecasted vs Actual Wearing-off for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - actual vs forecast.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions with f1 score, precision, recall, and accuracy\n",
    "fpr, tpr, thresholds = metrics.roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                         forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "######################\n",
    "model_metric_scores = pd.DataFrame(\n",
    "  [\n",
    "    metrics.f1_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.recall_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.precision_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.accuracy_score(\n",
    "      forecasts_output.ground_truth,\n",
    "      forecasts_output.forecasted_wearing_off),\n",
    "    metrics.auc(fpr, tpr),\n",
    "    metrics.average_precision_score(\n",
    "      forecasts_output.sort_index().ground_truth,\n",
    "      forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "  ],\n",
    "  index=['f1 score', 'recall', 'precision', 'accuracy', 'auc-roc', 'auc-prc'],\n",
    "  columns=['metrics']\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "model_metric_scores.set_index(['participant', 'model'], inplace=True)\n",
    "\n",
    "######################\n",
    "model_classification_report = pd.DataFrame(\n",
    "  classification_report(\n",
    "    forecasts_output.ground_truth,\n",
    "    forecasts_output.forecasted_wearing_off,\n",
    "    output_dict=True\n",
    "  )\n",
    ").T.round(3).assign(model=model_name, participant=USER)\n",
    "# Set index's name to 'classification report'\n",
    "model_classification_report.index.name = 'classification report'\n",
    "\n",
    "# Remove row that has 'accuracy' as index\n",
    "model_classification_report = model_classification_report.drop(\n",
    "  ['accuracy'], axis=0)\n",
    "\n",
    "model_classification_report = model_classification_report.reset_index()\n",
    "model_classification_report.set_index(\n",
    "    ['participant', 'model', 'classification report'], inplace=True)\n",
    "\n",
    "model_metric_scores.reset_index(inplace=True)\n",
    "model_classification_report.reset_index(inplace=True)\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "model_metric_scores.to_excel(excel_writer=writer, sheet_name='Metric Scores',\n",
    "                             startrow=writer.sheets['Metric Scores'].max_row,\n",
    "                             header=False, index=False)\n",
    "model_classification_report.to_excel(excel_writer=writer, sheet_name='Classification Report',\n",
    "                                     startrow=writer.sheets['Classification Report'].max_row,\n",
    "                                     header=False, index=False)\n",
    "writer.close()\n",
    "\n",
    "display(model_metric_scores)\n",
    "display(model_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['No Wearing-off', 'Wearing-off']\n",
    "conf_matrix = confusion_matrix(forecasts_output.ground_truth,\n",
    "                               forecasts_output.forecasted_wearing_off)\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "sns.heatmap(conf_matrix,\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            annot=True, fmt=\".2f\", cmap='Blues_r')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True class')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s confusion matrix for {USER.upper()}\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - confusion matrix.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "# Recreate writer to open existing file\n",
    "writer = pd.ExcelWriter(f'{RESULTS_PATH}/metric scores.xlsx',\n",
    "                        engine='openpyxl', mode='a', if_sheet_exists='overlay')\n",
    "\n",
    "# Append data frame to Metric Scores sheet\n",
    "pd.DataFrame(\n",
    "  data=[[USER, model_name] + list(conf_matrix.flatten())],\n",
    "  columns=['participant', 'model', 'TN', 'FP', 'FN', 'TP']\n",
    ").to_excel(excel_writer=writer, sheet_name='Confusion Matrix',\n",
    "           startrow=writer.sheets['Confusion Matrix'].max_row,\n",
    "           header=False, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                 forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s ROC curve for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - ROC curve.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU-PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and graph PRC and AU-PRC\n",
    "precision, recall, thresholds = precision_recall_curve(forecasts_output.sort_index().ground_truth,\n",
    "                                                       forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "average_precision = average_precision_score(\n",
    "  forecasts_output.sort_index().ground_truth,\n",
    "  forecasts_output.sort_index().forecasted_wearing_off_probability)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE_CM)\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=2, label='PRC curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([0, 1], [0, 1], color='navy',\n",
    "         lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Precision')\n",
    "\n",
    "# Set title\n",
    "plt.title(f\"{model_name}'s PRC for {USER.upper()}\")\n",
    "\n",
    "# Set legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f'{RESULTS_PATH}/{USER}, {model_name} - PRC.png',\n",
    "            bbox_inches='tight', dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
